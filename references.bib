
@article{battcock_acoustically_2019,
	title = {Acoustically {Expressing} {Affect}},
	volume = {37},
	issn = {0730-7829, 1533-8312},
	url = {https://online.ucpress.edu/mp/article/37/1/66/62853/Acoustically-Expressing-Affect},
	doi = {10.1525/mp.2019.37.1.66},
	abstract = {Composers convey emotion through music by co-varying structural cues. Although the complex interplay provides a rich listening experience, this creates challenges for understanding the contributions of individual cues. Here we investigate how three specific cues (attack rate, mode, and pitch height) work together to convey emotion in Bach's Well Tempered-Clavier (WTC). In three experiments, we explore responses to (1) eight-measure excerpts and (2) musically “resolved” excerpts, and (3) investigate the role of different standard dimensional scales of emotion. In each experiment, thirty nonmusician participants rated perceived emotion along scales of valence and intensity (Experiments 1 \& 2) or valence and arousal (Experiment 3) for 48 pieces in the WTC. Responses indicate listeners used attack rate, Mode, and pitch height to make judgements of valence, but only attack rate for intensity/arousal. Commonality analyses revealed mode predicted the most variance for valence ratings, followed by attack rate, with pitch height contributing minimally. In Experiment 2 mode increased in predictive power compared to Experiment 1. For Experiment 3, using “arousal” instead of “intensity” showed similar results to Experiment 1. We discuss how these results complement and extend previous findings of studies with tightly controlled stimuli, providing additional perspective on complex issues of interpersonal communication.},
	language = {en},
	number = {1},
	urldate = {2021-09-28},
	journal = {Music Perception},
	author = {Battcock, Aimee and Schutz, Michael},
	month = sep,
	year = {2019},
	pages = {66--91},
	file = {Battcock and Schutz - 2019 - Acoustically Expressing Affect.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\5ZW7QT6U\\Battcock and Schutz - 2019 - Acoustically Expressing Affect.pdf:application/pdf},
}

@misc{yesiler_less_2020,
	title = {Less is more: {Faster} and better music version identification with embedding distillation},
	shorttitle = {Less is more},
	url = {http://arxiv.org/abs/2010.03284},
	abstract = {Version identiﬁcation systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made signiﬁcant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99\% smaller embeddings that, moreover, yield up to a 3\% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Yesiler, Furkan and Serrà, Joan and Gómez, Emilia},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03284 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Yesiler et al. - 2020 - Less is more Faster and better music version iden.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\PN2T6E8A\\Yesiler et al. - 2020 - Less is more Faster and better music version iden.pdf:application/pdf},
}

@article{anderson_exploring_2022,
	title = {Exploring historic changes in musical communication: {Deconstructing} emotional cues in preludes by {Bach} and {Chopin}},
	volume = {50},
	issn = {0305-7356, 1741-3087},
	shorttitle = {Exploring historic changes in musical communication},
	url = {http://journals.sagepub.com/doi/10.1177/03057356211046375},
	doi = {10.1177/03057356211046375},
	abstract = {A growing body of research analyzing musical scores suggests mode’s relationship with other expressive cues has changed over time. However, to the best of our knowledge, the perceptual implications of these changes have not been formally assessed. Here, we explore how compositional choices of 17th- and 19th-century composers (J. S. Bach and F. Chopin, respectively) differentially affect emotional communication. This novel exploration builds on our team’s previous techniques using commonality analysis to decompose intercorrelated cues in unaltered excerpts of influential compositions. In doing so, we offer an important naturalistic complement to traditional experimental work—often involving tightly controlled stimuli constructed to avoid the intercorrelations inherent to naturalistic music. Our data indicate intriguing changes in cues’ effects between Bach and Chopin, consistent with score-based research suggesting mode’s “meaning” changed across historical eras. For example, mode’s unique effect accounts for the most variance in valence ratings of Chopin’s preludes, whereas its shared use with attack rate plays a more prominent role in Bach’s. We discuss the implications of these findings as part of our field’s ongoing effort to understand the complexity of musical communication—addressing issues only visible when moving beyond stimuli created for scientific, rather than artistic, goals.},
	language = {en},
	number = {5},
	urldate = {2022-09-20},
	journal = {Psychology of Music},
	author = {Anderson, Cameron J. and Schutz, Michael},
	month = sep,
	year = {2022},
	pages = {1424--1442},
	file = {Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\QUBUBSNM\\Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:application/pdf},
}

@article{battcock_emotion_2022,
	title = {Emotion and expertise: how listeners with formal music training use cues to perceive emotion},
	volume = {86},
	issn = {0340-0727, 1430-2772},
	shorttitle = {Emotion and expertise},
	url = {https://link.springer.com/10.1007/s00426-020-01467-1},
	doi = {10.1007/s00426-020-01467-1},
	abstract = {Although studies of musical emotion often focus on the role of the composer and performer, the communicative process is also influenced by the listener’s musical background or experience. Given the equivocal nature of evidence regarding the effects of musical training, the role of listener expertise in conveyed musical emotion remains opaque. Here we examine emotional responses of musically trained listeners across two experiments using (1) eight measure excerpts, (2) musically resolved excerpts and compare them to responses collected from untrained listeners in Battcock and Schutz (2019). In each experiment 30 participants with six or more years of music training rated perceived emotion for 48 excerpts from Bach’s WellTempered Clavier (WTC) using scales of valence and arousal. Models of listener ratings predict more variance in trained vs. untrained listeners across both experiments. More importantly however, we observe a shift in cue weights related to training. Using commonality analysis and Fischer Z score comparisons as well as margin of error calculations, we show that timing and mode affect untrained listeners equally, whereas mode plays a significantly stronger role than timing for trained listeners. This is not to say the emotional messages are less well recognized by untrained listeners—simply that training appears to shift the relative weight of cues used in making evaluations. These results clarify music training’s potential impact on the specific effects of cues in conveying musical emotion.},
	language = {en},
	number = {1},
	urldate = {2022-09-20},
	journal = {Psychological Research},
	author = {Battcock, Aimee and Schutz, Michael},
	month = feb,
	year = {2022},
	pages = {66--86},
	file = {Battcock and Schutz - 2022 - Emotion and expertise how listeners with formal m.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YCXGF2LN\\Battcock and Schutz - 2022 - Emotion and expertise how listeners with formal m.pdf:application/pdf},
}

@article{battcock_individualized_2021,
	title = {Individualized interpretation: {Exploring} structural and interpretive effects on evaluations of emotional content in {Bach}’s {Well} {Tempered} {Clavier}},
	volume = {50},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Individualized interpretation},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2021.1979050},
	doi = {10.1080/09298215.2021.1979050},
	abstract = {Audiences, juries, and critics continually evaluate performers based on their interpretations of familiar classics. Yet formally assessing the perceptual consequences of interpretive decisions is challenging – particularly with respect to how they shape emotional messages. Here, we explore the issue through comparison of emotion ratings (using scales of arousal and valence) for excerpts of all 48 pieces from Bach’s Well-Tempered Clavier. In this series of studies, participants evaluated one of seven interpretations by highly regarded pianists. This work offers the novel ability to simultaneously explore (1) how different interpretations by expert pianists shape emotional messages, (2) the degree to which structural and interpretative elements shape the clarity of emotional messages, and (3) how interpretative differences affect the strength of specific features or cues to convey musical emotion.},
	language = {en},
	number = {5},
	urldate = {2022-09-20},
	journal = {Journal of New Music Research},
	author = {Battcock, Aimee and Schutz, Michael},
	month = oct,
	year = {2021},
	pages = {447--468},
	file = {Battcock and Schutz - 2021 - Individualized interpretation Exploring structura.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JGP98PTD\\Battcock and Schutz - 2021 - Individualized interpretation Exploring structura.pdf:application/pdf},
}

@article{gomez_tonal_2006,
	title = {Tonal {Description} of {Polyphonic} {Audio} for {Music} {Content} {Processing}},
	volume = {18},
	issn = {1091-9856, 1526-5528},
	url = {http://pubsonline.informs.org/doi/10.1287/ijoc.1040.0126},
	doi = {10.1287/ijoc.1040.0126},
	abstract = {We present a method to extract a description of the tonal aspects of music from polyphonic audio signals. We define this tonal description using different levels of abstraction, differentiating between low-level signal descriptors and high-level textual labels. We also establish different temporal scales for description, defining some features as being attached to a certain time instant, and other global descriptors as related to a wider segment. The description is validated by estimating the key of a piece. We also propose the description as a tonal representation of the polyphonic audio signal to measure tonal similarity between audio excerpts and to establish the tonal structure of a musical piece.},
	language = {en},
	number = {3},
	urldate = {2022-09-20},
	journal = {INFORMS Journal on Computing},
	author = {Gómez, Emilia},
	month = aug,
	year = {2006},
	pages = {294--304},
	file = {Gómez - 2006 - Tonal Description of Polyphonic Audio for Music Co.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\HDHICZJD\\Gómez - 2006 - Tonal Description of Polyphonic Audio for Music Co.pdf:application/pdf},
}

@article{kelly_exploring_2021,
	title = {Exploring {Changes} in the {Emotional} {Classification} of {Music} between {Eras}},
	volume = {4},
	issn = {2574-2442, 2574-2450},
	url = {https://www.tandfonline.com/doi/full/10.1080/25742442.2021.1988422},
	doi = {10.1080/25742442.2021.1988422},
	language = {en},
	number = {1-2},
	urldate = {2022-09-20},
	journal = {Auditory Perception \& Cognition},
	author = {Kelly, Benjamin O and Anderson, Cameron J and Schutz, Michael},
	month = apr,
	year = {2021},
	pages = {121--131},
	file = {Kelly et al. - 2021 - Exploring Changes in the Emotional Classification .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JX46UUIK\\Kelly et al. - 2021 - Exploring Changes in the Emotional Classification .pdf:application/pdf},
}

@article{petri_exploring_2009,
	title = {Exploring relationships between audio features and emotion in music},
	volume = {3},
	issn = {1662-5161},
	url = {http://www.frontiersin.org/10.3389/conf.neuro.09.2009.02.033/event_abstract},
	doi = {10.3389/conf.neuro.09.2009.02.033},
	abstract = {In this paper, we present an analysis of the associations between emotion categories and audio features automatically extracted from raw audio data. This work is based on 110 excerpts from film soundtracks evaluated by 116 listeners. This data is annotated with 5 basic emotions (fear, anger, happiness, sadness, tenderness) on a 7 points scale. Exploiting state-of-the-art Music Information Retrieval (MIR) techniques, we extract audio features of different kind: timbral, rhythmic and tonal. Among others we also compute estimations of dissonance, mode, onset rate and loudness. We study statistical relations between audio descriptors and emotion categories confirming results from psychological studies. We also use machine-learning techniques to model the emotion ratings. We create regression models based on the Support Vector Regression algorithm that can estimate the ratings with a correlation of 0.65 in average.},
	language = {en},
	urldate = {2022-09-20},
	journal = {Frontiers in Human Neuroscience},
	author = {Petri, Toiviaine},
	year = {2009},
	file = {Petri - 2009 - Exploring relationships between audio features and.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JALLM2XR\\Petri - 2009 - Exploring relationships between audio features and.pdf:application/pdf},
}

@article{muller_towards_2010,
	title = {Towards {Timbre}-{Invariant} {Audio} {Features} for {Harmony}-{Based} {Music}},
	volume = {18},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5410051/},
	doi = {10.1109/TASL.2010.2041394},
	abstract = {Chroma-based audio features are a well-established tool for analyzing and comparing harmony-based Western music that is based on the equal-tempered scale. By identifying spectral components that differ by a musical octave, chroma features possess a considerable amount of robustness to changes in timbre and instrumentation. In this paper, we describe a novel procedure that further enhances chroma features by signiﬁcantly boosting the degree of timbre invariance without degrading the features’ discriminative power. Our idea is based on the generally accepted observation that the lower mel-frequency cepstral coefﬁcients (MFCCs) are closely related to timbre. Now, instead of keeping the lower coefﬁcients, we discard them and only keep the upper coefﬁcients. Furthermore, using a pitch scale instead of a mel scale allows us to project the remaining coefﬁcients onto the 12 chroma bins. We present a series of experiments to demonstrate that the resulting chroma features outperform various state-of-the art features in the context of music matching and retrieval applications. As a ﬁnal contribution, we give a detailed analysis of our enhancement procedure revealing the musical meaning of certain pitch-frequency cepstral coefﬁcients.},
	language = {en},
	number = {3},
	urldate = {2022-09-20},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Muller, Meinard and Ewert, Sebastian},
	month = mar,
	year = {2010},
	pages = {649--662},
	file = {Muller and Ewert - 2010 - Towards Timbre-Invariant Audio Features for Harmon.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\766TFKVS\\Muller and Ewert - 2010 - Towards Timbre-Invariant Audio Features for Harmon.pdf:application/pdf},
}

@article{poon_cueing_2015,
	title = {Cueing musical emotions: {An} empirical analysis of 24-piece sets by {Bach} and {Chopin} documents parallels with emotional speech},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Cueing musical emotions},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.01419/abstract},
	doi = {10.3389/fpsyg.2015.01419},
	abstract = {Acoustic cues such as pitch height and timing are effective at communicating emotion in both music and speech. Numerous experiments altering musical passages have shown that higher and faster melodies generally sound “happier” than lower and slower melodies, ﬁndings consistent with corpus analyses of emotional speech. However, equivalent corpus analyses of complex time-varying cues in music are less common, due in part to the challenges of assembling an appropriate corpus. Here, we describe a novel, score-based exploration of the use of pitch height and timing in a set of “balanced” major and minor key compositions. Our analysis included all 24 Preludes and 24 Fugues from Bach’s Well-Tempered Clavier (book 1), as well as all 24 of Chopin’s Preludes for piano. These three sets are balanced with respect to both modality (major/minor) and key chroma (“A,” “B,” “C,” etc.). Consistent with predictions derived from speech, we found major-key (nominally “happy”) pieces to be two semitones higher in pitch height and 29\% faster than minor-key (nominally “sad”) pieces. This demonstrates that our balanced corpus of major and minor key pieces uses low-level acoustic cues for emotion in a manner consistent with speech. A series of post hoc analyses illustrate interesting trade-offs, with sets featuring greater emphasis on timing distinctions between modalities exhibiting the least pitch distinction, and vice-versa. We discuss these ﬁndings in the broader context of speech-music research, as well as recent scholarship exploring the historical evolution of cue use in Western music.},
	language = {en},
	urldate = {2022-09-20},
	journal = {Frontiers in Psychology},
	author = {Poon, Matthew and Schutz, Michael},
	month = nov,
	year = {2015},
	file = {Poon and Schutz - 2015 - Cueing musical emotions An empirical analysis of .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JNR9J4V9\\Poon and Schutz - 2015 - Cueing musical emotions An empirical analysis of .pdf:application/pdf},
}

@article{salamon_tonal_2013,
	title = {Tonal representations for music retrieval: from version identification to query-by-humming},
	volume = {2},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Tonal representations for music retrieval},
	url = {http://link.springer.com/10.1007/s13735-012-0026-0},
	doi = {10.1007/s13735-012-0026-0},
	language = {en},
	number = {1},
	urldate = {2022-09-20},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Salamon, Justin and Serrà, Joan and Gómez, Emilia},
	month = mar,
	year = {2013},
	pages = {45--58},
	file = {Salamon et al. - 2013 - Tonal representations for music retrieval from ve.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\798FDJZS\\Salamon et al. - 2013 - Tonal representations for music retrieval from ve.pdf:application/pdf},
}

@book{schoeffmann_multimedia_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MultiMedia} {Modeling}},
	volume = {10704},
	isbn = {978-3-319-73602-0 978-3-319-73603-7},
	url = {http://link.springer.com/10.1007/978-3-319-73603-7},
	language = {en},
	urldate = {2022-09-20},
	publisher = {Springer International Publishing},
	editor = {Schoeffmann, Klaus and Chalidabhongse, Thanarat H. and Ngo, Chong Wah and Aramvith, Supavadee and O’Connor, Noel E. and Ho, Yo-Sung and Gabbouj, Moncef and Elgammal, Ahmed},
	year = {2018},
	doi = {10.1007/978-3-319-73603-7},
	file = {Schoeffmann et al. - 2018 - MultiMedia Modeling.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\U4ZHDG48\\Schoeffmann et al. - 2018 - MultiMedia Modeling.pdf:application/pdf},
}

@article{serra_chroma_2008,
	title = {Chroma {Binary} {Similarity} and {Local} {Alignment} {Applied} to {Cover} {Song} {Identification}},
	volume = {16},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4523006/},
	doi = {10.1109/TASL.2008.924595},
	abstract = {We present a new technique for audio signal comparison based on tonal subsequence alignment and its application to detect cover versions (i.e., different performances of the same underlying musical piece). Cover song identiﬁcation is a task whose popularity has increased in the music information retrieval (MIR) community along in the past, as it provides a direct and objective way to evaluate music similarity algorithms. This paper ﬁrst presents a series of experiments carried out with two state-of-the-art methods for cover song identiﬁcation. We have studied several components of these (such as chroma resolution and similarity, transposition, beat tracking or dynamic time warping constraints), in order to discover which characteristics would be desirable for a competitive cover song identiﬁer. After analyzing many cross-validated results, the importance of these characteristics is discussed, and the best performing ones are ﬁnally applied to the newly proposed method. Multiple evaluations of this one conﬁrm a large increase in identiﬁcation accuracy when comparing it with alternative state-of-the-art approaches.},
	language = {en},
	number = {6},
	urldate = {2022-09-20},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Serra, J. and Gomez, E. and Herrera, P. and Serra, X.},
	month = aug,
	year = {2008},
	pages = {1138--1151},
	file = {Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\QJJ7HFQ6\\Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf:application/pdf},
}

@incollection{kacprzyk_audio_2010,
	address = {Berlin, Heidelberg},
	title = {Audio {Cover} {Song} {Identification} and {Similarity}: {Background}, {Approaches}, {Evaluation}, and {Beyond}},
	volume = {274},
	isbn = {978-3-642-11673-5 978-3-642-11674-2},
	shorttitle = {Audio {Cover} {Song} {Identification} and {Similarity}},
	url = {http://link.springer.com/10.1007/978-3-642-11674-2_14},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {Advances in {Music} {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Serrà, Joan and Gómez, Emilia and Herrera, Perfecto},
	editor = {Kacprzyk, Janusz and Raś, Zbigniew W. and Wieczorkowska, Alicja A.},
	year = {2010},
	doi = {10.1007/978-3-642-11674-2_14},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {307--332},
	file = {Serrà et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\XB5WFHUB\\Serrà et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf:application/pdf},
}

@article{tzanetakis_pitch_2003,
	title = {Pitch {Histograms} in {Audio} and {Symbolic} {Music} {Information} {Retrieval}},
	volume = {32},
	issn = {0929-8215},
	url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.2.143.16743},
	doi = {10.1076/jnmr.32.2.143.16743},
	abstract = {In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques.},
	language = {en},
	number = {2},
	urldate = {2022-09-20},
	journal = {Journal of New Music Research},
	author = {Tzanetakis, George and Ermolinskyi, Andrey and Cook, Perry},
	month = jun,
	year = {2003},
	pages = {143--152},
	file = {Tzanetakis et al. - 2003 - Pitch Histograms in Audio and Symbolic Music Infor.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\CBVZ2VEC\\Tzanetakis et al. - 2003 - Pitch Histograms in Audio and Symbolic Music Infor.pdf:application/pdf},
}

@article{yi-hsuan_yang_ranking-based_2011,
	title = {Ranking-{Based} {Emotion} {Recognition} for {Music} {Organization} and {Retrieval}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5545401/},
	doi = {10.1109/TASL.2010.2064164},
	abstract = {Determining the emotion of a song that best characterizes the affective content of the song is a challenging issue due to the difﬁculty of collecting reliable ground truth data and the semantic gap between human’s perception and the music signal of the song. To address this issue, we represent an emotion as a point in the Cartesian space with valence and arousal as the dimensions and determine the coordinates of a song by the relative emotion of the song with respect to other songs. We also develop an RBF-ListNet algorithm to optimize the ranking-based objective function of our approach. The cognitive load of annotation, the accuracy of emotion recognition, and the subjective quality of the proposed approach are extensively evaluated. Experimental results show that this ranking-based approach simpliﬁes emotion annotation and enhances the reliability of the ground truth. The performance of our algorithm for valence recognition reaches 0.326 in Gamma statistic.},
	language = {en},
	number = {4},
	urldate = {2022-09-20},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {{Yi-Hsuan Yang} and Chen, Homer H},
	month = may,
	year = {2011},
	pages = {762--774},
	file = {Yi-Hsuan Yang and Chen - 2011 - Ranking-Based Emotion Recognition for Music Organi.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\2T6ZVDP9\\Yi-Hsuan Yang and Chen - 2011 - Ranking-Based Emotion Recognition for Music Organi.pdf:application/pdf},
}

@article{yang_machine_2012,
	title = {Machine {Recognition} of {Music} {Emotion}: {A} {Review}},
	volume = {3},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Machine {Recognition} of {Music} {Emotion}},
	url = {https://dl.acm.org/doi/10.1145/2168752.2168754},
	doi = {10.1145/2168752.2168754},
	abstract = {The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research.},
	language = {en},
	number = {3},
	urldate = {2022-09-20},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Yang, Yi-Hsuan and Chen, Homer H.},
	month = may,
	year = {2012},
	pages = {1--30},
	file = {Yang and Chen - 2012 - Machine Recognition of Music Emotion A Review.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\MVUFMM2F\\Yang and Chen - 2012 - Machine Recognition of Music Emotion A Review.pdf:application/pdf},
}

@misc{lartillot_mirtoolbox_2021,
	title = {{MIRtoolbox} 1.8.1 {User} {Manual}},
	publisher = {RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion},
	author = {Lartillot, Olivier},
	year = {2021},
	file = {MIRtoolbox_manual1.8.1.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Z7Q8E264\\MIRtoolbox_manual1.8.1.pdf:application/pdf;ReleaseHistory.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\U64458SR\\ReleaseHistory.pdf:application/pdf},
}

@incollection{preisach_matlab_2008,
	address = {Berlin, Heidelberg},
	title = {A {Matlab} {Toolbox} for {Music} {Information} {Retrieval}},
	isbn = {978-3-540-78239-1 978-3-540-78246-9},
	url = {http://link.springer.com/10.1007/978-3-540-78246-9_31},
	abstract = {We present MIRToolbox, an integrated set of functions written in Matlab, dedicated to the extraction from audio ﬁles of musical features related, among others, to timbre, tonality, rhythm or form. The objective is to oﬀer a state of the art of computational approaches in the area of Music Information Retrieval (MIR). The design is based on a modular framework: the diﬀerent algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating diﬀerent variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize. These functions can adapt to a large area of objects as input.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {Data {Analysis}, {Machine} {Learning} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lartillot, Olivier and Toiviainen, Petri and Eerola, Tuomas},
	editor = {Preisach, Christine and Burkhardt, Hans and Schmidt-Thieme, Lars and Decker, Reinhold},
	year = {2008},
	doi = {10.1007/978-3-540-78246-9_31},
	note = {Series Title: Studies in Classification, Data Analysis, and Knowledge Organization},
	pages = {261--268},
	file = {Lartillot et al. - 2008 - A Matlab Toolbox for Music Information Retrieval.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\XQX28YPJ\\Lartillot et al. - 2008 - A Matlab Toolbox for Music Information Retrieval.pdf:application/pdf},
}

@article{lartillot_matlab_2007,
	title = {A {Matlab} {Toolbox} for {Musical} {Feature} {Extraction} from {Audio}},
	abstract = {We present MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio ﬁles. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize.},
	language = {en},
	author = {Lartillot, Olivier and Toiviainen, Petri},
	year = {2007},
	pages = {8},
	file = {Lartillot and Toiviainen - 2007 - A Matlab Toolbox for Musical Feature Extraction fr.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\HR9RIFNV\\Lartillot and Toiviainen - 2007 - A Matlab Toolbox for Musical Feature Extraction fr.pdf:application/pdf},
}

@inproceedings{kumar_testing_2015,
	address = {Coimbatore, India},
	title = {Testing reliability of {Mirtoolbox}},
	isbn = {978-1-4799-7225-8},
	url = {http://ieeexplore.ieee.org/document/7125004/},
	doi = {10.1109/ECS.2015.7125004},
	abstract = {In the present work, reliability of the software Mirtoolbox that was developed for music signal processing has been investigated into. Motivation for the work stems from the fact that performance report of the software is not available in the literature from sources other than the developers of the software. A few functions available in the software for extraction of some basic musical features have been experimented with. Results show that the performance of the software varies depending on the musical feature being extracted and the instrument being played.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {2015 2nd {International} {Conference} on {Electronics} and {Communication} {Systems} ({ICECS})},
	publisher = {IEEE},
	author = {Kumar, Neeraj and Kumar, Raubin and Bhattacharya, Subrata},
	month = feb,
	year = {2015},
	pages = {710--717},
	file = {Kumar et al. - 2015 - Testing reliability of Mirtoolbox.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\GAZQXPZM\\Kumar et al. - 2015 - Testing reliability of Mirtoolbox.pdf:application/pdf},
}

@inproceedings{wang_analysis_2013,
	address = {Chengdu, China},
	title = {The {Analysis} and {Comparison} of {Vital} {Acoustic} {Features} in {Content}-{Based} {Classification} of {Music} {Genre}},
	isbn = {978-1-4799-2877-4 978-1-4799-2876-7},
	url = {http://ieeexplore.ieee.org/document/6710015/},
	doi = {10.1109/ITA.2013.99},
	abstract = {Digital music is becoming increasingly popular in the Internet, and content-based musical genre classification has gained significant attentions in the field of musical retrieval. In this paper, the acoustic musical features are extracted from the viewpoints of both signal processing and the musical dimension. By comparing the performance of classifier of different combination of acoustic features, the contributions of corresponding features are evaluated. Finally, timbre and tonality feature sets are found to be the most effective features in music genre recognition.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {2013 {International} {Conference} on {Information} {Technology} and {Applications}},
	publisher = {IEEE},
	author = {Wang, Zhe and Xia, Jingbo and Luo, Bin},
	month = nov,
	year = {2013},
	pages = {404--408},
	file = {Wang et al. - 2013 - The Analysis and Comparison of Vital Acoustic Feat.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\MQ9NQZYP\\Wang et al. - 2013 - The Analysis and Comparison of Vital Acoustic Feat.pdf:application/pdf},
}

@book{bach_well_1983,
	address = {[United States], Van Nuys, CA},
	title = {The well tempered clavier: 48 preludes and fugues. {Volume} 1},
	isbn = {978-0-7692-8572-6},
	shorttitle = {The well tempered clavier},
	language = {zxx},
	publisher = {Kalmus ; Alfred Pub. Co.},
	author = {Bach, Johann Sebastian and Bischoff, Hans},
	year = {1983},
	note = {OCLC: 848953741},
	file = {bachjs_wtc1_bischoff_preludes.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\KREY5W8M\\bachjs_wtc1_bischoff_preludes.pdf:application/pdf},
}

@book{bach_well-tempered_2004,
	address = {Van Nuys, Calif.},
	edition = {3rd ed},
	title = {The well-tempered clavier. {Volume} 1},
	isbn = {978-0-88284-831-0},
	language = {eng},
	publisher = {Alfred Pub. Co.},
	author = {Bach, Johann Sebastian and Palmer, Willard A.},
	year = {2004},
	note = {OCLC: 988404992},
	file = {Palmer table of tempi for Bach-1 and Bach-2.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\67V7UNGM\\Palmer table of tempi for Bach-1 and Bach-2.pdf:application/pdf},
}

@article{cuthbert_music21_2010,
	title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
	abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (scorebased) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills.},
	language = {en},
	author = {Cuthbert, Michael Scott and Ariza, Christopher},
	year = {2010},
	pages = {7},
	file = {Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Z3UN2IGP\\Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf},
}

@book{krumhansl_cognitive_2001,
	address = {New York, NY},
	edition = {1. issued paperb},
	series = {Oxford psychology series},
	title = {Cognitive foundations of musical pitch},
	isbn = {978-0-19-514836-7},
	language = {eng},
	number = {17},
	publisher = {Oxford Univ. Press},
	author = {Krumhansl, Carol L.},
	year = {2001},
}
