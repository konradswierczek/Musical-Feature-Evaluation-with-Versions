
@article{lieck_tonal_2020,
	title = {The {Tonal} {Diffusion} {Model}},
	volume = {3},
	issn = {2514-3298},
	url = {https://transactions.ismir.net/article/10.5334/tismir.46/},
	doi = {10.5334/tismir.46},
	language = {en},
	number = {1},
	urldate = {2021-09-28},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Lieck, Robert and Moss, Fabian C. and Rohrmeier, Martin},
	month = oct,
	year = {2020},
	pages = {153},
	file = {Lieck et al. - 2020 - The Tonal Diffusion Model.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YFYT5MGP\\Lieck et al. - 2020 - The Tonal Diffusion Model.pdf:application/pdf},
}

@article{battcock_acoustically_2019,
	title = {Acoustically {Expressing} {Affect}},
	volume = {37},
	issn = {0730-7829, 1533-8312},
	url = {https://online.ucpress.edu/mp/article/37/1/66/62853/Acoustically-Expressing-Affect},
	doi = {10.1525/mp.2019.37.1.66},
	abstract = {Composers convey emotion through music by co-varying structural cues. Although the complex interplay provides a rich listening experience, this creates challenges for understanding the contributions of individual cues. Here we investigate how three specific cues (attack rate, mode, and pitch height) work together to convey emotion in Bach's Well Tempered-Clavier (WTC). In three experiments, we explore responses to (1) eight-measure excerpts and (2) musically “resolved” excerpts, and (3) investigate the role of different standard dimensional scales of emotion. In each experiment, thirty nonmusician participants rated perceived emotion along scales of valence and intensity (Experiments 1 \& 2) or valence and arousal (Experiment 3) for 48 pieces in the WTC. Responses indicate listeners used attack rate, Mode, and pitch height to make judgements of valence, but only attack rate for intensity/arousal. Commonality analyses revealed mode predicted the most variance for valence ratings, followed by attack rate, with pitch height contributing minimally. In Experiment 2 mode increased in predictive power compared to Experiment 1. For Experiment 3, using “arousal” instead of “intensity” showed similar results to Experiment 1. We discuss how these results complement and extend previous findings of studies with tightly controlled stimuli, providing additional perspective on complex issues of interpersonal communication.},
	language = {en},
	number = {1},
	urldate = {2021-09-28},
	journal = {Music Perception},
	author = {Battcock, Aimee and Schutz, Michael},
	month = sep,
	year = {2019},
	pages = {66--91},
	file = {Battcock and Schutz - 2019 - Acoustically Expressing Affect.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\5ZW7QT6U\\Battcock and Schutz - 2019 - Acoustically Expressing Affect.pdf:application/pdf},
}

@article{temperley_whats_1999,
	title = {What's {Key} for {Key}? {The} {Krumhansl}-{Schmuckler} {Key}-{Finding} {Algorithm} {Reconsidered}},
	volume = {17},
	issn = {0730-7829},
	shorttitle = {What's {Key} for {Key}?},
	url = {https://online.ucpress.edu/mp/article/17/1/65/62051/Whats-Key-for-Key-The-KrumhanslSchmuckler},
	doi = {10.2307/40285812},
	abstract = {This study examines the Krumhansl-Schmuckler key-finding model, in which the distribution of pitch classes in a piece is compared with an ideal distribution or "key profile" for each key. Several changes are proposed. First, the formula used for the matching process is somewhat simplified. Second, alternative values are proposed for the key profiles themselves. Third, rather than summing the durations of all events of each pitch class, the revised model divides the piece into short segments and labels each pitch class as present or absent in each segment. Fourth, a mechanism for modulation is proposed; a penalty is imposed for changing key from one segment to the next. An implementation of this model was subjected to two tests. First, the model was tested on the fugue subjects from Bach's Well-Tempered Clavier; the model's performance on this corpus is compared with the performances of other models. Second, the model was tested on a corpus of excerpts from the Kostka and Payne harmony textbook (as analyzed by Kostka). Several problems with the modified algorithm are discussed, concerning the rate of modulation, the role of harmony in key finding, and the role of pitch "spellings." The model is also compared with Huron and Parncutt's exponential decay model. The tests presented here suggest that the key-profile model, with the modifications proposed, can provide a highly successful approach to key finding.},
	language = {en},
	number = {1},
	urldate = {2021-09-28},
	journal = {Music Perception},
	author = {Temperley, David},
	month = oct,
	year = {1999},
	note = {This is an extra},
	keywords = {This is a tag},
	pages = {65--100},
	file = {Temperley - 1999 - What's Key for Key The Krumhansl-Schmuckler Key-F.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\QDY7M734\\Temperley - 1999 - What's Key for Key The Krumhansl-Schmuckler Key-F.pdf:application/pdf},
}

@misc{yesiler_less_2020,
	title = {Less is more: {Faster} and better music version identification with embedding distillation},
	shorttitle = {Less is more},
	url = {http://arxiv.org/abs/2010.03284},
	abstract = {Version identiﬁcation systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made signiﬁcant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99\% smaller embeddings that, moreover, yield up to a 3\% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.},
	language = {en},
	urldate = {2022-09-20},
	publisher = {arXiv},
	author = {Yesiler, Furkan and Serrà, Joan and Gómez, Emilia},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03284 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Yesiler et al. - 2020 - Less is more Faster and better music version iden.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\PN2T6E8A\\Yesiler et al. - 2020 - Less is more Faster and better music version iden.pdf:application/pdf},
}

@article{anderson_exploring_2022,
	title = {Exploring historic changes in musical communication: {Deconstructing} emotional cues in preludes by {Bach} and {Chopin}},
	volume = {50},
	issn = {0305-7356, 1741-3087},
	shorttitle = {Exploring historic changes in musical communication},
	url = {http://journals.sagepub.com/doi/10.1177/03057356211046375},
	doi = {10.1177/03057356211046375},
	abstract = {A growing body of research analyzing musical scores suggests mode’s relationship with other expressive cues has changed over time. However, to the best of our knowledge, the perceptual implications of these changes have not been formally assessed. Here, we explore how compositional choices of 17th- and 19th-century composers (J. S. Bach and F. Chopin, respectively) differentially affect emotional communication. This novel exploration builds on our team’s previous techniques using commonality analysis to decompose intercorrelated cues in unaltered excerpts of influential compositions. In doing so, we offer an important naturalistic complement to traditional experimental work—often involving tightly controlled stimuli constructed to avoid the intercorrelations inherent to naturalistic music. Our data indicate intriguing changes in cues’ effects between Bach and Chopin, consistent with score-based research suggesting mode’s “meaning” changed across historical eras. For example, mode’s unique effect accounts for the most variance in valence ratings of Chopin’s preludes, whereas its shared use with attack rate plays a more prominent role in Bach’s. We discuss the implications of these findings as part of our field’s ongoing effort to understand the complexity of musical communication—addressing issues only visible when moving beyond stimuli created for scientific, rather than artistic, goals.},
	language = {en},
	number = {5},
	urldate = {2022-09-20},
	journal = {Psychology of Music},
	author = {Anderson, Cameron J. and Schutz, Michael},
	month = sep,
	year = {2022},
	pages = {1424--1442},
	file = {Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\QUBUBSNM\\Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:application/pdf},
}

@article{battcock_emotion_2022,
	title = {Emotion and expertise: how listeners with formal music training use cues to perceive emotion},
	volume = {86},
	issn = {0340-0727, 1430-2772},
	shorttitle = {Emotion and expertise},
	url = {https://link.springer.com/10.1007/s00426-020-01467-1},
	doi = {10.1007/s00426-020-01467-1},
	abstract = {Although studies of musical emotion often focus on the role of the composer and performer, the communicative process is also influenced by the listener’s musical background or experience. Given the equivocal nature of evidence regarding the effects of musical training, the role of listener expertise in conveyed musical emotion remains opaque. Here we examine emotional responses of musically trained listeners across two experiments using (1) eight measure excerpts, (2) musically resolved excerpts and compare them to responses collected from untrained listeners in Battcock and Schutz (2019). In each experiment 30 participants with six or more years of music training rated perceived emotion for 48 excerpts from Bach’s WellTempered Clavier (WTC) using scales of valence and arousal. Models of listener ratings predict more variance in trained vs. untrained listeners across both experiments. More importantly however, we observe a shift in cue weights related to training. Using commonality analysis and Fischer Z score comparisons as well as margin of error calculations, we show that timing and mode affect untrained listeners equally, whereas mode plays a significantly stronger role than timing for trained listeners. This is not to say the emotional messages are less well recognized by untrained listeners—simply that training appears to shift the relative weight of cues used in making evaluations. These results clarify music training’s potential impact on the specific effects of cues in conveying musical emotion.},
	language = {en},
	number = {1},
	urldate = {2022-09-20},
	journal = {Psychological Research},
	author = {Battcock, Aimee and Schutz, Michael},
	month = feb,
	year = {2022},
	pages = {66--86},
	file = {Battcock and Schutz - 2022 - Emotion and expertise how listeners with formal m.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YCXGF2LN\\Battcock and Schutz - 2022 - Emotion and expertise how listeners with formal m.pdf:application/pdf},
}

@article{battcock_individualized_2021,
	title = {Individualized interpretation: {Exploring} structural and interpretive effects on evaluations of emotional content in {Bach}’s {Well} {Tempered} {Clavier}},
	volume = {50},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Individualized interpretation},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2021.1979050},
	doi = {10.1080/09298215.2021.1979050},
	abstract = {Audiences, juries, and critics continually evaluate performers based on their interpretations of familiar classics. Yet formally assessing the perceptual consequences of interpretive decisions is challenging – particularly with respect to how they shape emotional messages. Here, we explore the issue through comparison of emotion ratings (using scales of arousal and valence) for excerpts of all 48 pieces from Bach’s Well-Tempered Clavier. In this series of studies, participants evaluated one of seven interpretations by highly regarded pianists. This work offers the novel ability to simultaneously explore (1) how different interpretations by expert pianists shape emotional messages, (2) the degree to which structural and interpretative elements shape the clarity of emotional messages, and (3) how interpretative differences affect the strength of specific features or cues to convey musical emotion.},
	language = {en},
	number = {5},
	urldate = {2022-09-20},
	journal = {Journal of New Music Research},
	author = {Battcock, Aimee and Schutz, Michael},
	month = oct,
	year = {2021},
	pages = {447--468},
	file = {Battcock and Schutz - 2021 - Individualized interpretation Exploring structura.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JGP98PTD\\Battcock and Schutz - 2021 - Individualized interpretation Exploring structura.pdf:application/pdf;MAUCHTheAudioDegradation2013Published.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\TPEJEYMT\\MAUCHTheAudioDegradation2013Published.pdf:application/pdf},
}

@article{gomez_tonal_2006,
	title = {Tonal {Description} of {Polyphonic} {Audio} for {Music} {Content} {Processing}},
	volume = {18},
	issn = {1091-9856, 1526-5528},
	url = {http://pubsonline.informs.org/doi/10.1287/ijoc.1040.0126},
	doi = {10.1287/ijoc.1040.0126},
	abstract = {We present a method to extract a description of the tonal aspects of music from polyphonic audio signals. We define this tonal description using different levels of abstraction, differentiating between low-level signal descriptors and high-level textual labels. We also establish different temporal scales for description, defining some features as being attached to a certain time instant, and other global descriptors as related to a wider segment. The description is validated by estimating the key of a piece. We also propose the description as a tonal representation of the polyphonic audio signal to measure tonal similarity between audio excerpts and to establish the tonal structure of a musical piece.},
	language = {en},
	number = {3},
	urldate = {2022-09-20},
	journal = {INFORMS Journal on Computing},
	author = {Gómez, Emilia},
	month = aug,
	year = {2006},
	pages = {294--304},
	file = {Gómez - 2006 - Tonal Description of Polyphonic Audio for Music Co.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\HDHICZJD\\Gómez - 2006 - Tonal Description of Polyphonic Audio for Music Co.pdf:application/pdf},
}

@article{kelly_exploring_2021,
	title = {Exploring {Changes} in the {Emotional} {Classification} of {Music} between {Eras}},
	volume = {4},
	issn = {2574-2442, 2574-2450},
	url = {https://www.tandfonline.com/doi/full/10.1080/25742442.2021.1988422},
	doi = {10.1080/25742442.2021.1988422},
	language = {en},
	number = {1-2},
	urldate = {2022-09-20},
	journal = {Auditory Perception \& Cognition},
	author = {Kelly, Benjamin O and Anderson, Cameron J and Schutz, Michael},
	month = apr,
	year = {2021},
	pages = {121--131},
	file = {Kelly et al. - 2021 - Exploring Changes in the Emotional Classification .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JX46UUIK\\Kelly et al. - 2021 - Exploring Changes in the Emotional Classification .pdf:application/pdf},
}

@article{petri_exploring_2009,
	title = {Exploring relationships between audio features and emotion in music},
	volume = {3},
	issn = {1662-5161},
	url = {http://www.frontiersin.org/10.3389/conf.neuro.09.2009.02.033/event_abstract},
	doi = {10.3389/conf.neuro.09.2009.02.033},
	abstract = {In this paper, we present an analysis of the associations between emotion categories and audio features automatically extracted from raw audio data. This work is based on 110 excerpts from film soundtracks evaluated by 116 listeners. This data is annotated with 5 basic emotions (fear, anger, happiness, sadness, tenderness) on a 7 points scale. Exploiting state-of-the-art Music Information Retrieval (MIR) techniques, we extract audio features of different kind: timbral, rhythmic and tonal. Among others we also compute estimations of dissonance, mode, onset rate and loudness. We study statistical relations between audio descriptors and emotion categories confirming results from psychological studies. We also use machine-learning techniques to model the emotion ratings. We create regression models based on the Support Vector Regression algorithm that can estimate the ratings with a correlation of 0.65 in average.},
	language = {en},
	urldate = {2022-09-20},
	journal = {Frontiers in Human Neuroscience},
	author = {Petri, Toiviaine},
	year = {2009},
	file = {Petri - 2009 - Exploring relationships between audio features and.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JALLM2XR\\Petri - 2009 - Exploring relationships between audio features and.pdf:application/pdf},
}

@article{muller_towards_2010,
	title = {Towards {Timbre}-{Invariant} {Audio} {Features} for {Harmony}-{Based} {Music}},
	volume = {18},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5410051/},
	doi = {10.1109/TASL.2010.2041394},
	abstract = {Chroma-based audio features are a well-established tool for analyzing and comparing harmony-based Western music that is based on the equal-tempered scale. By identifying spectral components that differ by a musical octave, chroma features possess a considerable amount of robustness to changes in timbre and instrumentation. In this paper, we describe a novel procedure that further enhances chroma features by signiﬁcantly boosting the degree of timbre invariance without degrading the features’ discriminative power. Our idea is based on the generally accepted observation that the lower mel-frequency cepstral coefﬁcients (MFCCs) are closely related to timbre. Now, instead of keeping the lower coefﬁcients, we discard them and only keep the upper coefﬁcients. Furthermore, using a pitch scale instead of a mel scale allows us to project the remaining coefﬁcients onto the 12 chroma bins. We present a series of experiments to demonstrate that the resulting chroma features outperform various state-of-the art features in the context of music matching and retrieval applications. As a ﬁnal contribution, we give a detailed analysis of our enhancement procedure revealing the musical meaning of certain pitch-frequency cepstral coefﬁcients.},
	language = {en},
	number = {3},
	urldate = {2022-09-20},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Muller, Meinard and Ewert, Sebastian},
	month = mar,
	year = {2010},
	pages = {649--662},
	file = {Muller and Ewert - 2010 - Towards Timbre-Invariant Audio Features for Harmon.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\766TFKVS\\Muller and Ewert - 2010 - Towards Timbre-Invariant Audio Features for Harmon.pdf:application/pdf},
}

@article{poon_cueing_2015,
	title = {Cueing musical emotions: {An} empirical analysis of 24-piece sets by {Bach} and {Chopin} documents parallels with emotional speech},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Cueing musical emotions},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.01419/abstract},
	doi = {10.3389/fpsyg.2015.01419},
	abstract = {Acoustic cues such as pitch height and timing are effective at communicating emotion in both music and speech. Numerous experiments altering musical passages have shown that higher and faster melodies generally sound “happier” than lower and slower melodies, ﬁndings consistent with corpus analyses of emotional speech. However, equivalent corpus analyses of complex time-varying cues in music are less common, due in part to the challenges of assembling an appropriate corpus. Here, we describe a novel, score-based exploration of the use of pitch height and timing in a set of “balanced” major and minor key compositions. Our analysis included all 24 Preludes and 24 Fugues from Bach’s Well-Tempered Clavier (book 1), as well as all 24 of Chopin’s Preludes for piano. These three sets are balanced with respect to both modality (major/minor) and key chroma (“A,” “B,” “C,” etc.). Consistent with predictions derived from speech, we found major-key (nominally “happy”) pieces to be two semitones higher in pitch height and 29\% faster than minor-key (nominally “sad”) pieces. This demonstrates that our balanced corpus of major and minor key pieces uses low-level acoustic cues for emotion in a manner consistent with speech. A series of post hoc analyses illustrate interesting trade-offs, with sets featuring greater emphasis on timing distinctions between modalities exhibiting the least pitch distinction, and vice-versa. We discuss these ﬁndings in the broader context of speech-music research, as well as recent scholarship exploring the historical evolution of cue use in Western music.},
	language = {en},
	urldate = {2022-09-20},
	journal = {Frontiers in Psychology},
	author = {Poon, Matthew and Schutz, Michael},
	month = nov,
	year = {2015},
	file = {Poon and Schutz - 2015 - Cueing musical emotions An empirical analysis of .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JNR9J4V9\\Poon and Schutz - 2015 - Cueing musical emotions An empirical analysis of .pdf:application/pdf},
}

@article{salamon_tonal_2013,
	title = {Tonal representations for music retrieval: from version identification to query-by-humming},
	volume = {2},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Tonal representations for music retrieval},
	url = {http://link.springer.com/10.1007/s13735-012-0026-0},
	doi = {10.1007/s13735-012-0026-0},
	language = {en},
	number = {1},
	urldate = {2022-09-20},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Salamon, Justin and Serrà, Joan and Gómez, Emilia},
	month = mar,
	year = {2013},
	pages = {45--58},
	file = {Salamon et al. - 2013 - Tonal representations for music retrieval from ve.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\798FDJZS\\Salamon et al. - 2013 - Tonal representations for music retrieval from ve.pdf:application/pdf},
}

@book{schoeffmann_multimedia_2018,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MultiMedia} {Modeling}},
	volume = {10704},
	isbn = {978-3-319-73602-0 978-3-319-73603-7},
	url = {http://link.springer.com/10.1007/978-3-319-73603-7},
	language = {en},
	urldate = {2022-09-20},
	publisher = {Springer International Publishing},
	editor = {Schoeffmann, Klaus and Chalidabhongse, Thanarat H. and Ngo, Chong Wah and Aramvith, Supavadee and O’Connor, Noel E. and Ho, Yo-Sung and Gabbouj, Moncef and Elgammal, Ahmed},
	year = {2018},
	doi = {10.1007/978-3-319-73603-7},
	file = {Schoeffmann et al. - 2018 - MultiMedia Modeling.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\U4ZHDG48\\Schoeffmann et al. - 2018 - MultiMedia Modeling.pdf:application/pdf},
}

@article{serra_chroma_2008,
	title = {Chroma {Binary} {Similarity} and {Local} {Alignment} {Applied} to {Cover} {Song} {Identification}},
	volume = {16},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4523006/},
	doi = {10.1109/TASL.2008.924595},
	abstract = {We present a new technique for audio signal comparison based on tonal subsequence alignment and its application to detect cover versions (i.e., different performances of the same underlying musical piece). Cover song identiﬁcation is a task whose popularity has increased in the music information retrieval (MIR) community along in the past, as it provides a direct and objective way to evaluate music similarity algorithms. This paper ﬁrst presents a series of experiments carried out with two state-of-the-art methods for cover song identiﬁcation. We have studied several components of these (such as chroma resolution and similarity, transposition, beat tracking or dynamic time warping constraints), in order to discover which characteristics would be desirable for a competitive cover song identiﬁer. After analyzing many cross-validated results, the importance of these characteristics is discussed, and the best performing ones are ﬁnally applied to the newly proposed method. Multiple evaluations of this one conﬁrm a large increase in identiﬁcation accuracy when comparing it with alternative state-of-the-art approaches.},
	language = {en},
	number = {6},
	urldate = {2022-09-20},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Serra, J. and Gomez, E. and Herrera, P. and Serra, X.},
	month = aug,
	year = {2008},
	pages = {1138--1151},
	file = {Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\QJJ7HFQ6\\Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf:application/pdf},
}

@incollection{kacprzyk_audio_2010,
	address = {Berlin, Heidelberg},
	title = {Audio {Cover} {Song} {Identification} and {Similarity}: {Background}, {Approaches}, {Evaluation}, and {Beyond}},
	volume = {274},
	isbn = {978-3-642-11673-5 978-3-642-11674-2},
	shorttitle = {Audio {Cover} {Song} {Identification} and {Similarity}},
	url = {http://link.springer.com/10.1007/978-3-642-11674-2_14},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {Advances in {Music} {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Serrà, Joan and Gómez, Emilia and Herrera, Perfecto},
	editor = {Kacprzyk, Janusz and Raś, Zbigniew W. and Wieczorkowska, Alicja A.},
	year = {2010},
	doi = {10.1007/978-3-642-11674-2_14},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {307--332},
	file = {Serrà et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\XB5WFHUB\\Serrà et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf:application/pdf},
}

@article{tzanetakis_pitch_2003,
	title = {Pitch {Histograms} in {Audio} and {Symbolic} {Music} {Information} {Retrieval}},
	volume = {32},
	issn = {0929-8215},
	url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.2.143.16743},
	doi = {10.1076/jnmr.32.2.143.16743},
	abstract = {In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques.},
	language = {en},
	number = {2},
	urldate = {2022-09-20},
	journal = {Journal of New Music Research},
	author = {Tzanetakis, George and Ermolinskyi, Andrey and Cook, Perry},
	month = jun,
	year = {2003},
	pages = {143--152},
	file = {Tzanetakis et al. - 2003 - Pitch Histograms in Audio and Symbolic Music Infor.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\CBVZ2VEC\\Tzanetakis et al. - 2003 - Pitch Histograms in Audio and Symbolic Music Infor.pdf:application/pdf},
}

@article{yi-hsuan_yang_ranking-based_2011,
	title = {Ranking-{Based} {Emotion} {Recognition} for {Music} {Organization} and {Retrieval}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5545401/},
	doi = {10.1109/TASL.2010.2064164},
	abstract = {Determining the emotion of a song that best characterizes the affective content of the song is a challenging issue due to the difﬁculty of collecting reliable ground truth data and the semantic gap between human’s perception and the music signal of the song. To address this issue, we represent an emotion as a point in the Cartesian space with valence and arousal as the dimensions and determine the coordinates of a song by the relative emotion of the song with respect to other songs. We also develop an RBF-ListNet algorithm to optimize the ranking-based objective function of our approach. The cognitive load of annotation, the accuracy of emotion recognition, and the subjective quality of the proposed approach are extensively evaluated. Experimental results show that this ranking-based approach simpliﬁes emotion annotation and enhances the reliability of the ground truth. The performance of our algorithm for valence recognition reaches 0.326 in Gamma statistic.},
	language = {en},
	number = {4},
	urldate = {2022-09-20},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {{Yi-Hsuan Yang} and Chen, Homer H},
	month = may,
	year = {2011},
	pages = {762--774},
	file = {Yi-Hsuan Yang and Chen - 2011 - Ranking-Based Emotion Recognition for Music Organi.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\2T6ZVDP9\\Yi-Hsuan Yang and Chen - 2011 - Ranking-Based Emotion Recognition for Music Organi.pdf:application/pdf},
}

@article{yang_machine_2012,
	title = {Machine {Recognition} of {Music} {Emotion}: {A} {Review}},
	volume = {3},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Machine {Recognition} of {Music} {Emotion}},
	url = {https://dl.acm.org/doi/10.1145/2168752.2168754},
	doi = {10.1145/2168752.2168754},
	abstract = {The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research.},
	language = {en},
	number = {3},
	urldate = {2022-09-20},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Yang, Yi-Hsuan and Chen, Homer H.},
	month = may,
	year = {2012},
	pages = {1--30},
	file = {Yang and Chen - 2012 - Machine Recognition of Music Emotion A Review.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\MVUFMM2F\\Yang and Chen - 2012 - Machine Recognition of Music Emotion A Review.pdf:application/pdf},
}

@misc{lartillot_mirtoolbox_2021,
	title = {{MIRtoolbox} 1.8.1 {User} {Manual}},
	publisher = {RITMO Centre for Interdisciplinary Studies in Rhythm, Time and Motion},
	author = {Lartillot, Olivier},
	year = {2021},
	file = {MIRtoolbox_manual1.8.1.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Z7Q8E264\\MIRtoolbox_manual1.8.1.pdf:application/pdf;ReleaseHistory.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\U64458SR\\ReleaseHistory.pdf:application/pdf},
}

@incollection{preisach_matlab_2008,
	address = {Berlin, Heidelberg},
	title = {A {Matlab} {Toolbox} for {Music} {Information} {Retrieval}},
	isbn = {978-3-540-78239-1 978-3-540-78246-9},
	url = {http://link.springer.com/10.1007/978-3-540-78246-9_31},
	abstract = {We present MIRToolbox, an integrated set of functions written in Matlab, dedicated to the extraction from audio ﬁles of musical features related, among others, to timbre, tonality, rhythm or form. The objective is to oﬀer a state of the art of computational approaches in the area of Music Information Retrieval (MIR). The design is based on a modular framework: the diﬀerent algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating diﬀerent variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize. These functions can adapt to a large area of objects as input.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {Data {Analysis}, {Machine} {Learning} and {Applications}},
	publisher = {Springer Berlin Heidelberg},
	author = {Lartillot, Olivier and Toiviainen, Petri and Eerola, Tuomas},
	editor = {Preisach, Christine and Burkhardt, Hans and Schmidt-Thieme, Lars and Decker, Reinhold},
	year = {2008},
	doi = {10.1007/978-3-540-78246-9_31},
	note = {Series Title: Studies in Classification, Data Analysis, and Knowledge Organization},
	pages = {261--268},
	file = {Lartillot et al. - 2008 - A Matlab Toolbox for Music Information Retrieval.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\XQX28YPJ\\Lartillot et al. - 2008 - A Matlab Toolbox for Music Information Retrieval.pdf:application/pdf},
}

@article{lartillot_matlab_2007,
	title = {A {Matlab} {Toolbox} for {Musical} {Feature} {Extraction} from {Audio}},
	abstract = {We present MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio ﬁles. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize.},
	language = {en},
	author = {Lartillot, Olivier and Toiviainen, Petri},
	year = {2007},
	pages = {8},
	file = {Lartillot and Toiviainen - 2007 - A Matlab Toolbox for Musical Feature Extraction fr.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\HR9RIFNV\\Lartillot and Toiviainen - 2007 - A Matlab Toolbox for Musical Feature Extraction fr.pdf:application/pdf},
}

@inproceedings{kumar_testing_2015,
	address = {Coimbatore, India},
	title = {Testing reliability of {Mirtoolbox}},
	isbn = {978-1-4799-7225-8},
	url = {http://ieeexplore.ieee.org/document/7125004/},
	doi = {10.1109/ECS.2015.7125004},
	abstract = {In the present work, reliability of the software Mirtoolbox that was developed for music signal processing has been investigated into. Motivation for the work stems from the fact that performance report of the software is not available in the literature from sources other than the developers of the software. A few functions available in the software for extraction of some basic musical features have been experimented with. Results show that the performance of the software varies depending on the musical feature being extracted and the instrument being played.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {2015 2nd {International} {Conference} on {Electronics} and {Communication} {Systems} ({ICECS})},
	publisher = {IEEE},
	author = {Kumar, Neeraj and Kumar, Raubin and Bhattacharya, Subrata},
	month = feb,
	year = {2015},
	pages = {710--717},
	file = {Kumar et al. - 2015 - Testing reliability of Mirtoolbox.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\GAZQXPZM\\Kumar et al. - 2015 - Testing reliability of Mirtoolbox.pdf:application/pdf},
}

@inproceedings{wang_analysis_2013,
	address = {Chengdu, China},
	title = {The {Analysis} and {Comparison} of {Vital} {Acoustic} {Features} in {Content}-{Based} {Classification} of {Music} {Genre}},
	isbn = {978-1-4799-2877-4 978-1-4799-2876-7},
	url = {http://ieeexplore.ieee.org/document/6710015/},
	doi = {10.1109/ITA.2013.99},
	abstract = {Digital music is becoming increasingly popular in the Internet, and content-based musical genre classification has gained significant attentions in the field of musical retrieval. In this paper, the acoustic musical features are extracted from the viewpoints of both signal processing and the musical dimension. By comparing the performance of classifier of different combination of acoustic features, the contributions of corresponding features are evaluated. Finally, timbre and tonality feature sets are found to be the most effective features in music genre recognition.},
	language = {en},
	urldate = {2022-09-20},
	booktitle = {2013 {International} {Conference} on {Information} {Technology} and {Applications}},
	publisher = {IEEE},
	author = {Wang, Zhe and Xia, Jingbo and Luo, Bin},
	month = nov,
	year = {2013},
	pages = {404--408},
	file = {Wang et al. - 2013 - The Analysis and Comparison of Vital Acoustic Feat.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\MQ9NQZYP\\Wang et al. - 2013 - The Analysis and Comparison of Vital Acoustic Feat.pdf:application/pdf},
}

@book{bach_well_1983,
	address = {[United States], Van Nuys, CA},
	title = {The well tempered clavier: 48 preludes and fugues. {Volume} 1},
	isbn = {978-0-7692-8572-6},
	shorttitle = {The well tempered clavier},
	language = {zxx},
	publisher = {Kalmus ; Alfred Pub. Co.},
	author = {Bach, Johann Sebastian and Bischoff, Hans},
	year = {1983},
	note = {OCLC: 848953741},
	file = {bachjs_wtc1_bischoff_preludes.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\KREY5W8M\\bachjs_wtc1_bischoff_preludes.pdf:application/pdf},
}

@book{bach_well-tempered_2004,
	address = {Van Nuys, Calif.},
	edition = {3rd ed},
	title = {The well-tempered clavier. {Volume} 1},
	isbn = {978-0-88284-831-0},
	language = {eng},
	publisher = {Alfred Pub. Co.},
	author = {Bach, Johann Sebastian and Palmer, Willard A.},
	year = {2004},
	note = {OCLC: 988404992},
	file = {2007.14714.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\KV6BADTF\\2007.14714.pdf:application/pdf;2301.01578.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\9P6DYEJW\\2301.01578.pdf:application/pdf;Palmer table of tempi for Bach-1 and Bach-2.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\67V7UNGM\\Palmer table of tempi for Bach-1 and Bach-2.pdf:application/pdf;Vatolkin.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\RBB29RPZ\\Vatolkin.pdf:application/pdf},
}

@article{cuthbert_music21_2010,
	title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
	abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (scorebased) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills.},
	language = {en},
	author = {Cuthbert, Michael Scott and Ariza, Christopher},
	year = {2010},
	pages = {7},
	file = {Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Z3UN2IGP\\Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf},
}

@book{krumhansl_cognitive_2001,
	address = {New York, NY},
	edition = {1. issued paperb},
	series = {Oxford psychology series},
	title = {Cognitive foundations of musical pitch},
	isbn = {978-0-19-514836-7},
	language = {eng},
	number = {17},
	publisher = {Oxford Univ. Press},
	author = {Krumhansl, Carol L.},
	year = {2001},
}

@article{bogdanov_essentia_nodate,
	title = {{ESSENTIA}: {AN} {AUDIO} {ANALYSIS} {LIBRARY} {FOR} {MUSIC} {INFORMATION} {RETRIEVAL}},
	abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predeﬁned executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, speciﬁcally the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
	language = {en},
	author = {Bogdanov, Dmitry and Wack, Nicolas and Gomez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, Jose and Serra, Xavier},
	file = {Bogdanov et al. - ESSENTIA AN AUDIO ANALYSIS LIBRARY FOR MUSIC INFO.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\RY36XZT2\\Bogdanov et al. - ESSENTIA AN AUDIO ANALYSIS LIBRARY FOR MUSIC INFO.pdf:application/pdf},
}

@inproceedings{mcfee_librosa_2015,
	address = {Austin, Texas},
	title = {librosa: {Audio} and {Music} {Signal} {Analysis} in {Python}},
	shorttitle = {librosa},
	url = {https://conference.scipy.org/proceedings/scipy2015/brian_mcfee.html},
	doi = {10.25080/Majora-7b98e3ed-003},
	abstract = {This document describes version 0.4.0 of librosa: a Python package for audio and music signal processing. At a high level, librosa provides implementations of a variety of common functions used throughout the ﬁeld of music information retrieval. In this document, a brief overview of the library’s functionality is provided, along with explanations of the design goals, software development practices, and notational conventions.},
	language = {en},
	urldate = {2023-08-21},
	author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
	year = {2015},
	pages = {18--24},
	file = {McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\MAVWDHIF\\McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf:application/pdf},
}

@article{moffat_evaluation_2015,
	title = {An {Evaluation} of {Audio} {Feature} {Extraction} {Toolboxes}},
	abstract = {Audio feature extraction underpins a massive proportion of audio processing, music information retrieval, audio effect design and audio synthesis. Design, analysis, synthesis and evaluation often rely on audio features, but there are a large and diverse range of feature extraction tools presented to the community. An evaluation of existing audio feature extraction libraries was undertaken. Ten libraries and toolboxes were evaluated with the Cranﬁeld Model for evaluation of information retrieval systems, reviewing the coverage, effort, presentation and time lag of a system. Comparisons are undertaken of these tools and example use cases are presented as to when toolboxes are most suitable. This paper allows a software engineer or researcher to quickly and easily select a suitable audio feature extraction toolbox.},
	language = {en},
	author = {Moffat, David and Ronan, David and Reiss, Joshua D},
	year = {2015},
	file = {Moffat et al. - 2015 - An Evaluation of Audio Feature Extraction Toolboxe.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\6QSY2WA6\\Moffat et al. - 2015 - An Evaluation of Audio Feature Extraction Toolboxe.pdf:application/pdf},
}

@inproceedings{bogdanov_essentia_2013,
	address = {Barcelona Spain},
	title = {{ESSENTIA}: an open-source library for sound and music analysis},
	isbn = {978-1-4503-2404-5},
	shorttitle = {{ESSENTIA}},
	url = {https://dl.acm.org/doi/10.1145/2502081.2502229},
	doi = {10.1145/2502081.2502229},
	abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Aﬀero GPL license. It contains an extensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level music descriptors. The library is also wrapped in Python and includes a number of predeﬁned executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, speciﬁcally the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
	language = {en},
	urldate = {2023-08-21},
	booktitle = {Proceedings of the 21st {ACM} international conference on {Multimedia}},
	publisher = {ACM},
	author = {Bogdanov, Dmitry and Wack, Nicolas and Gómez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, Gerard and Salamon, Justin and Zapata, José and Serra, Xavier},
	month = oct,
	year = {2013},
	pages = {855--858},
	file = {Bogdanov et al. - 2013 - ESSENTIA an open-source library for sound and mus.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\KH3LJT4K\\Bogdanov et al. - 2013 - ESSENTIA an open-source library for sound and mus.pdf:application/pdf},
}

@book{kahneman_noise_2021,
	address = {London, UK},
	title = {Noise: {A} {Flaw} in {Human} {Judgment}},
	abstract = {From the world-leaders in strategic thinking and the multi-million copy bestselling authors of Thinking Fast and Slow and Nudge, the next big book to change the way you think. Wherever there is human judgment, there is noise. Imagine that two doctors in the same city give different diagnoses to identical patients – or that two judges in the same court give different sentences to people who have committed matching crimes. Now imagine that the same doctor and the same judge make different decisions depending on whether it is morning or afternoon, or Monday rather than Wednesday, or they haven’t yet had lunch. These are examples of noise: variability in judgments that should be identical. In Noise, Daniel Kahneman, Olivier Sibony and Cass R. Sunstein show how noise produces errors in many fields, including in medicine, law, public health, economic forecasting, forensic science, child protection, creative strategy, performance review and hiring. And although noise can be found wherever people are making judgments and decisions, individuals and organizations alike commonly ignore its impact, at great cost. Packed with new ideas, and drawing on the same kind of sharp analysis and breadth of case study that made Thinking, Fast and Slow and Nudge international bestsellers, Noise explains how and why humans are so susceptible to noise and bias in decision-making. We all make bad judgments more than we think. With a few simple remedies, this groundbreaking book explores what we can do to make better ones.},
	publisher = {William Collins},
	author = {Kahneman, Daniel and Sibony, Olivier and Sunstein, Cass},
	year = {2021},
	note = {Pages: 454},
	keywords = {Eerola},
}

@misc{bach_bach_1963,
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {Book} {I} [{Recorded} by {R}. {Kirkpatrick}]},
	publisher = {[CD]. Germany: Polydor International},
	author = {Bach, Johann Sebastian},
	year = {1963},
	note = {Clavichord},
}

@incollection{bach_well-tempered_1963,
	address = {Germany},
	title = {The {Well}-tempered {Clavier}, {Part} 1 [{Recorded} by {R}. {Kirkpatrick}]},
	publisher = {[Vinyl]. Deutsche Grammophon},
	author = {Bach, Johann Sebastian},
	year = {1963},
	note = {Harpischord},
	keywords = {durham},
}

@incollection{bach_bach_1964,
	address = {USA},
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {Book} {One} and {Book} {Two} [{Recorded} by {M}. {Hamilton}]},
	publisher = {[Vinyl]. Everest},
	author = {Bach, Johann Sebastian},
	year = {1964},
	note = {Harpsichord},
	keywords = {durham},
}

@incollection{bach_bach_1973,
	address = {New York},
	title = {Bach: {The} {Well}-{Tempered} {Clavier}, {Book} {I} - {BWV} 846-869 [{Recorded} by {A}. {Newman}]},
	booktitle = {Bach: {The} {Well}-{Tempered} {Clavier}, {Book} {I}},
	publisher = {[Vinyl]. Columbia Records},
	author = {Bach, Johann Sebastian},
	year = {1973},
}

@incollection{bach_bach_1973-1,
	title = {Bach: {Well}-{Tempered} {Clavier} {Book} 1 [{Recorded} by {A}. {Newman}]},
	publisher = {[Vinyl]. Columbia Masterworks.},
	author = {Bach, Johann Sebastian},
	year = {1973},
	keywords = {durham},
}

@incollection{bach_bach_1987,
	address = {New York, USA},
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {Book} {I} [{Recorded} by {W}. {Landowska}]},
	publisher = {[CD]. BMG Music},
	author = {Bach, Johann Sebastian},
	year = {1987},
	keywords = {durham},
}

@incollection{bach_bach_1989,
	address = {New York, USA},
	title = {Bach: {The} {Well} {Tempered} {Clavier} {Book} {I} [{Recorded} by {G}. {Leonhardt}]},
	publisher = {[CD]. BMG Classics. (Original work published 1973)},
	author = {Bach, Johann Sebastian},
	year = {1989},
	keywords = {durham},
}

@incollection{bach_js_1992,
	address = {Germany},
	title = {J.{S}. {Bach}: {The} {Well}-{Tempered} {Clavier} {Das} {Wohltemperierte} {Klavier} [{Recorded} by {S}. {Richter}]},
	publisher = {[CD]. BMG Music. (Original work published 1970)},
	author = {Bach, Johann Sebastian},
	year = {1992},
	keywords = {durham},
}

@incollection{bach_well_1992,
	title = {The {Well} {Tempered} {Clavier}, {Book} {One} [{Recorded} by {J}. {Demus}]},
	publisher = {[CD]. MCA Records, Inc. (Original work published 1956)},
	author = {Bach, Johann Sebastian},
	year = {1992},
	keywords = {durham},
}

@incollection{bach_bach_1993,
	address = {New York},
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {I} [{Recorded} by {G}. {Gould}]},
	publisher = {[CD]. Sony Classical. (Original work published 1963, 1964, \& 1965)},
	author = {Bach, Johann Sebastian},
	year = {1993},
	note = {Publication Title: Bach: The Well-Tempered Clavier I},
	keywords = {durham},
	file = {Bach - 1965 - Bach The Well-Tempered Clavier, Book I - BWV 855 Recorded by G. Gould:C\:\\Users\\Konrad\\Zotero\\storage\\9H448CTN\\Bach - 1965 - Bach The Well-Tempered Clavier, Book I - BWV 855 Recorded by G. Gould.mp3:audio/mpeg},
}

@incollection{bach_well_1994,
	address = {New York, USA},
	title = {The {Well} {Tempered} {Clavier}, {Book} {One} [{Recorded} by {J}. {Martins}]},
	publisher = {[CD]. Labor Records. (Original work published 1964)},
	author = {Bach, Johann Sebastian},
	year = {1994},
	keywords = {durham},
}

@incollection{bach_bach_1995,
	address = {Villingen, Germany},
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {Book} {I} [{Recorded} by {F}. {Gulda}]},
	publisher = {[CD]. Decca Classics Production (Original work published 1972)},
	author = {Bach, Johann Sebastian},
	year = {1995},
	note = {Publication Title: Bach: The Well-Tempered Clavier, Bk I},
	keywords = {durham},
	file = {Gulda - 1973 - Bach The Well-Tempered Clavier, Book I - Prelude #24 in B minor, BWV 869 Recorded by F. Gulda:C\:\\Users\\Konrad\\Zotero\\storage\\Z9K569LW\\Gulda - 1973 - Bach The Well-Tempered Clavier, Book I - Prelude #24 in B minor, BWV 869 Recorded by F. Gulda.mp3:audio/mpeg},
}

@incollection{bach_bach_1999,
	address = {Abbey Road},
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {Book} 1 \& 2 [{Recorded} by {R}. {Tureck}]},
	publisher = {[CD]. Deutsche Grammonphon. (Original work published 1953)},
	author = {Bach, Johann Sebastian},
	year = {1999},
	keywords = {durham},
}

@incollection{bach_bach_2001,
	address = {USA},
	title = {Bach: {The} {Well}-{Tempered} {Clavier} {Book} {I}: two complete recordings on piano and harpsichord [{Recorded} by {A}. {Newman}]},
	publisher = {[CD]. KHAEON World Music, Inc},
	author = {Bach, Johann Sebastian},
	year = {2001},
	keywords = {durham},
}

@incollection{bach_bach_2006,
	address = {London, England},
	title = {Bach: {The} {Well} {Tempered} {Clavier} {Book} {I} [{Recorded} by {D}. {Barenboim}]},
	publisher = {[CD]. Warner Classics},
	author = {Bach, Johann Sebastian},
	year = {2006},
	keywords = {durham},
}

@incollection{bach_bach_2006-1,
	address = {London, England},
	title = {Bach: {The} {Well}-{Tempered} {Clavier}, {Book} {I} - {BWV} 855 [{Recorded} by {V}. {Ashkenazy}]},
	publisher = {[CD]. DECCA Record Company Limited},
	author = {Bach, Johann Sebastian},
	year = {2006},
	keywords = {durham},
}

@incollection{bach_well-tempered_2006,
	title = {The {Well}-{Tempered} {Clavier} - {BWV} 846–893 [{Recorded} by {M}. {Galling}]},
	publisher = {[CD]. Membran Music Ltd.},
	author = {Bach, Johann Sebastian},
	year = {2006},
	keywords = {durham},
}

@incollection{bach_bach_2007,
	address = {London},
	title = {Bach: {The} {Well} {Tempered} {Clavier} [{Recorded} by {E}. {Fischer}]},
	publisher = {[CD]. EMI Records Ltd. (Original work published 1989)},
	author = {Bach, Johann Sebastian},
	year = {2007},
	keywords = {durham},
}

@incollection{bach_well-tempered_2015,
	address = {London, England},
	title = {The {Well}-{Tempered} {Clavier} {Book} {I} [{Recorded} by {P}. {De} {Maria}]},
	publisher = {[CD]. DECCA Music Group Limited (Original work published 1722)},
	author = {Bach, Johann Sebastian},
	year = {2015},
	keywords = {durham},
}

@article{downie_scientific_2004,
	title = {The {Scientific} {Evaluation} of {Music} {Information} {Retrieval} {Systems}: {Foundations} and {Future}},
	volume = {28},
	issn = {0148-9267, 1531-5169},
	shorttitle = {The {Scientific} {Evaluation} of {Music} {Information} {Retrieval} {Systems}},
	url = {https://direct.mit.edu/comj/article/28/2/12-23/93881},
	doi = {10.1162/014892604323112211},
	language = {en},
	number = {2},
	urldate = {2023-08-23},
	journal = {Computer Music Journal},
	author = {Downie, J. Stephen},
	month = jun,
	year = {2004},
	pages = {12--23},
	file = {Downie - 2004 - The Scientific Evaluation of Music Information Ret.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\779NMWAT\\Downie - 2004 - The Scientific Evaluation of Music Information Ret.pdf:application/pdf},
}

@article{bogdanov_taming_nodate,
	title = {{TAMING} {WILD} {HORSES} {WITH} {ESSENTIA} {MUSIC} {EXTRACTOR}},
	abstract = {We overview the work done on automatic music audio description using the Essentia Music Extractor. We have successfully applied this tool to the analysis on a large scale in the AcousticBrainz project, in which we have been able to annotate over 3 million music recordings. We are challenged to constantly improve feature extraction algorithms and make our tools more useful for researchers. We reﬂect on the quality of our current data and identify how it can be improved in following versions to be in line with the essential goals of Music Information Retrieval of building ﬁnal usable systems. We call on the community for help and contribution.},
	language = {en},
	author = {Bogdanov, Dmitry and Porter, Alastair and Serra, Xavier},
	file = {Bogdanov et al. - TAMING WILD HORSES WITH ESSENTIA MUSIC EXTRACTOR.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Q5HPSPAC\\Bogdanov et al. - TAMING WILD HORSES WITH ESSENTIA MUSIC EXTRACTOR.pdf:application/pdf},
}

@article{schmuckler_perceptual_2005,
	title = {Perceptual {Tests} of an {Algorithm} for {Musical} {Key}-{Finding}.},
	volume = {31},
	issn = {1939-1277, 0096-1523},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0096-1523.31.5.1124},
	doi = {10.1037/0096-1523.31.5.1124},
	abstract = {Perceiving the tonality of a musical passage is a fundamental aspect of the experience of hearing music. Models for determining tonality have thus occupied a central place in music cognition research. Three experiments investigated 1 well-known model of tonal determination: the Krumhansl–Schmuckler key-finding algorithm. In Experiment 1, listeners’ percepts of tonality following short musical fragments derived from preludes by Bach and Chopin were compared with predictions of tonality produced by the algorithm; these predictions were very accurate for the Bach preludes but considerably less so for the Chopin preludes. Experiment 2 explored a subset of the Chopin preludes, finding that the algorithm could predict tonal percepts on a measure-by-measure basis. In Experiment 3, the algorithm predicted listeners’ percepts of tonal movement throughout a complete Chopin prelude. These studies support the viability of the Krumhansl–Schmuckler key-finding algorithm as well as a model of listeners’ tonal perceptions of musical passages.},
	language = {en},
	number = {5},
	urldate = {2023-09-01},
	journal = {Journal of Experimental Psychology: Human Perception and Performance},
	author = {Schmuckler, Mark A. and Tomovski, Robert},
	year = {2005},
	pages = {1124--1149},
	file = {Schmuckler and Tomovski - 2005 - Perceptual Tests of an Algorithm for Musical Key-F.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\EJVKM527\\Schmuckler and Tomovski - 2005 - Perceptual Tests of an Algorithm for Musical Key-F.pdf:application/pdf},
}

@article{sturm_horse_2016,
	title = {The “{Horse}” {Inside}: {Seeking} {Causes} {Behind} the {Behaviors} of {Music} {Content} {Analysis} {Systems}},
	volume = {14},
	issn = {1544-3574},
	shorttitle = {The “{Horse}” {Inside}},
	url = {https://dl.acm.org/doi/10.1145/2967507},
	doi = {10.1145/2967507},
	abstract = {Building systems that possess the sensitivity and intelligence to identify and describe high-level attributes in music audio signals continues to be an elusive goal but one that surely has broad and deep implications for a wide variety of applications. Hundreds of articles have so far been published toward this goal, and great progress appears to have been made. Some systems produce remarkable accuracies at recognizing high-level semantic concepts, such as music style, genre, and mood. However, it might be that these numbers do not mean what they seem. In this article, we take a state-of-the-art music content analysis system and investigate what causes it to achieve exceptionally high performance in a benchmark music audio dataset. We dissect the system to understand its operation, determine its sensitivities and limitations, and predict the kinds of knowledge it could and could not possess about music. We perform a series of experiments to illuminate what the system has actually learned to do and to what extent it is performing the intended music listening task. Our results demonstrate how the initial manifestation of music intelligence in this state of the art can be deceptive. Our work provides constructive directions toward developing music content analysis systems that can address the music information and creation needs of real-world users.},
	language = {en},
	number = {2},
	urldate = {2023-09-12},
	journal = {Computers in Entertainment},
	author = {Sturm, Bob L.},
	month = dec,
	year = {2016},
	pages = {1--32},
	file = {Sturm - 2016 - The “Horse” Inside Seeking Causes Behind the Beha.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\2A29XIEK\\Sturm - 2016 - The “Horse” Inside Seeking Causes Behind the Beha.pdf:application/pdf},
}

@article{sturm_simple_2014,
	title = {A {Simple} {Method} to {Determine} if a {Music} {Information} {Retrieval} {System} is a “{Horse}”},
	volume = {16},
	issn = {1520-9210, 1941-0077},
	url = {http://ieeexplore.ieee.org/document/6847693/},
	doi = {10.1109/TMM.2014.2330697},
	abstract = {We propose and demonstrate a simple method to explain the ﬁgure of merit (FoM) of a music information retrieval (MIR) system evaluated in a dataset, speciﬁcally, whether the FoM comes from the system using characteristics confounded with the “ground truth” of the dataset. Akin to the controlled experiments designed to test the supposed mathematical ability of the famous horse “Clever Hans,” we perform two experiments to show how three state-of-the-art MIR systems produce excellent FoM in spite of not using musical knowledge. This provides avenues for improving MIR systems, as well as their evaluation. We make available a reproducible research package so that others can apply the same method to evaluating other MIR systems.},
	language = {en},
	number = {6},
	urldate = {2023-09-12},
	journal = {IEEE Transactions on Multimedia},
	author = {Sturm, Bob L.},
	month = oct,
	year = {2014},
	pages = {1636--1644},
	file = {Sturm - 2014 - A Simple Method to Determine if a Music Informatio.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\B9BT4657\\Sturm - 2014 - A Simple Method to Determine if a Music Informatio.pdf:application/pdf},
}

@article{serra_chroma_2008-1,
	title = {Chroma {Binary} {Similarity} and {Local} {Alignment} {Applied} to {Cover} {Song} {Identification}},
	volume = {16},
	issn = {1558-7916},
	url = {http://ieeexplore.ieee.org/document/4523006/},
	doi = {10.1109/TASL.2008.924595},
	abstract = {We present a new technique for audio signal comparison based on tonal subsequence alignment and its application to detect cover versions (i.e., different performances of the same underlying musical piece). Cover song identiﬁcation is a task whose popularity has increased in the music information retrieval (MIR) community along in the past, as it provides a direct and objective way to evaluate music similarity algorithms. This paper ﬁrst presents a series of experiments carried out with two state-of-the-art methods for cover song identiﬁcation. We have studied several components of these (such as chroma resolution and similarity, transposition, beat tracking or dynamic time warping constraints), in order to discover which characteristics would be desirable for a competitive cover song identiﬁer. After analyzing many cross-validated results, the importance of these characteristics is discussed, and the best performing ones are ﬁnally applied to the newly proposed method. Multiple evaluations of this one conﬁrm a large increase in identiﬁcation accuracy when comparing it with alternative state-of-the-art approaches.},
	language = {en},
	number = {6},
	urldate = {2023-10-23},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Serra, J. and Gomez, E. and Herrera, P. and Serra, X.},
	month = aug,
	year = {2008},
	pages = {1138--1151},
	file = {Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\798IZ22J\\Serra et al. - 2008 - Chroma Binary Similarity and Local Alignment Appli.pdf:application/pdf},
}

@book{schoeffmann_multimedia_2018-1,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {{MultiMedia} {Modeling}},
	volume = {10704},
	isbn = {978-3-319-73602-0 978-3-319-73603-7},
	url = {http://link.springer.com/10.1007/978-3-319-73603-7},
	language = {en},
	urldate = {2023-10-23},
	publisher = {Springer International Publishing},
	editor = {Schoeffmann, Klaus and Chalidabhongse, Thanarat H. and Ngo, Chong Wah and Aramvith, Supavadee and O’Connor, Noel E. and Ho, Yo-Sung and Gabbouj, Moncef and Elgammal, Ahmed},
	year = {2018},
	doi = {10.1007/978-3-319-73603-7},
	file = {Schoeffmann et al. - 2018 - MultiMedia Modeling.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\DN2SLSEB\\Schoeffmann et al. - 2018 - MultiMedia Modeling.pdf:application/pdf},
}

@article{salamon_tonal_2013-1,
	title = {Tonal representations for music retrieval: from version identification to query-by-humming},
	volume = {2},
	issn = {2192-6611, 2192-662X},
	shorttitle = {Tonal representations for music retrieval},
	url = {http://link.springer.com/10.1007/s13735-012-0026-0},
	doi = {10.1007/s13735-012-0026-0},
	language = {en},
	number = {1},
	urldate = {2023-10-23},
	journal = {International Journal of Multimedia Information Retrieval},
	author = {Salamon, Justin and Serrà, Joan and Gómez, Emilia},
	month = mar,
	year = {2013},
	pages = {45--58},
	file = {Salamon et al. - 2013 - Tonal representations for music retrieval from ve.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\C4N3J8ID\\Salamon et al. - 2013 - Tonal representations for music retrieval from ve.pdf:application/pdf},
}

@article{poon_cueing_2015-1,
	title = {Cueing musical emotions: {An} empirical analysis of 24-piece sets by {Bach} and {Chopin} documents parallels with emotional speech},
	volume = {6},
	issn = {1664-1078},
	shorttitle = {Cueing musical emotions},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2015.01419/abstract},
	doi = {10.3389/fpsyg.2015.01419},
	abstract = {Acoustic cues such as pitch height and timing are effective at communicating emotion in both music and speech. Numerous experiments altering musical passages have shown that higher and faster melodies generally sound “happier” than lower and slower melodies, ﬁndings consistent with corpus analyses of emotional speech. However, equivalent corpus analyses of complex time-varying cues in music are less common, due in part to the challenges of assembling an appropriate corpus. Here, we describe a novel, score-based exploration of the use of pitch height and timing in a set of “balanced” major and minor key compositions. Our analysis included all 24 Preludes and 24 Fugues from Bach’s Well-Tempered Clavier (book 1), as well as all 24 of Chopin’s Preludes for piano. These three sets are balanced with respect to both modality (major/minor) and key chroma (“A,” “B,” “C,” etc.). Consistent with predictions derived from speech, we found major-key (nominally “happy”) pieces to be two semitones higher in pitch height and 29\% faster than minor-key (nominally “sad”) pieces. This demonstrates that our balanced corpus of major and minor key pieces uses low-level acoustic cues for emotion in a manner consistent with speech. A series of post hoc analyses illustrate interesting trade-offs, with sets featuring greater emphasis on timing distinctions between modalities exhibiting the least pitch distinction, and vice-versa. We discuss these ﬁndings in the broader context of speech-music research, as well as recent scholarship exploring the historical evolution of cue use in Western music.},
	language = {en},
	urldate = {2023-10-23},
	journal = {Frontiers in Psychology},
	author = {Poon, Matthew and Schutz, Michael},
	month = nov,
	year = {2015},
	file = {Poon and Schutz - 2015 - Cueing musical emotions An empirical analysis of .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Y2BXV67C\\Poon and Schutz - 2015 - Cueing musical emotions An empirical analysis of .pdf:application/pdf},
}

@article{lartillot_matlab_2007-1,
	title = {A {Matlab} {Toolbox} for {Musical} {Feature} {Extraction} from {Audio}},
	abstract = {We present MIRtoolbox, an integrated set of functions written in Matlab, dedicated to the extraction of musical features from audio ﬁles. The design is based on a modular framework: the different algorithms are decomposed into stages, formalized using a minimal set of elementary mechanisms, and integrating different variants proposed by alternative approaches – including new strategies we have developed –, that users can select and parametrize.},
	language = {en},
	author = {Lartillot, Olivier and Toiviainen, Petri},
	year = {2007},
	file = {Lartillot and Toiviainen - 2007 - A Matlab Toolbox for Musical Feature Extraction fr.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\DAJGNEXN\\Lartillot and Toiviainen - 2007 - A Matlab Toolbox for Musical Feature Extraction fr.pdf:application/pdf},
}

@article{muller_towards_2010-1,
	title = {Towards {Timbre}-{Invariant} {Audio} {Features} for {Harmony}-{Based} {Music}},
	volume = {18},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5410051/},
	doi = {10.1109/TASL.2010.2041394},
	abstract = {Chroma-based audio features are a well-established tool for analyzing and comparing harmony-based Western music that is based on the equal-tempered scale. By identifying spectral components that differ by a musical octave, chroma features possess a considerable amount of robustness to changes in timbre and instrumentation. In this paper, we describe a novel procedure that further enhances chroma features by signiﬁcantly boosting the degree of timbre invariance without degrading the features’ discriminative power. Our idea is based on the generally accepted observation that the lower mel-frequency cepstral coefﬁcients (MFCCs) are closely related to timbre. Now, instead of keeping the lower coefﬁcients, we discard them and only keep the upper coefﬁcients. Furthermore, using a pitch scale instead of a mel scale allows us to project the remaining coefﬁcients onto the 12 chroma bins. We present a series of experiments to demonstrate that the resulting chroma features outperform various state-of-the art features in the context of music matching and retrieval applications. As a ﬁnal contribution, we give a detailed analysis of our enhancement procedure revealing the musical meaning of certain pitch-frequency cepstral coefﬁcients.},
	language = {en},
	number = {3},
	urldate = {2023-10-23},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Muller, Meinard and Ewert, Sebastian},
	month = mar,
	year = {2010},
	pages = {649--662},
	file = {Muller and Ewert - 2010 - Towards Timbre-Invariant Audio Features for Harmon.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\PYIGKN7L\\Muller and Ewert - 2010 - Towards Timbre-Invariant Audio Features for Harmon.pdf:application/pdf},
}

@article{petri_exploring_2009-1,
	title = {Exploring relationships between audio features and emotion in music},
	volume = {3},
	issn = {1662-5161},
	url = {http://www.frontiersin.org/10.3389/conf.neuro.09.2009.02.033/event_abstract},
	doi = {10.3389/conf.neuro.09.2009.02.033},
	abstract = {In this paper, we present an analysis of the associations between emotion categories and audio features automatically extracted from raw audio data. This work is based on 110 excerpts from film soundtracks evaluated by 116 listeners. This data is annotated with 5 basic emotions (fear, anger, happiness, sadness, tenderness) on a 7 points scale. Exploiting state-of-the-art Music Information Retrieval (MIR) techniques, we extract audio features of different kind: timbral, rhythmic and tonal. Among others we also compute estimations of dissonance, mode, onset rate and loudness. We study statistical relations between audio descriptors and emotion categories confirming results from psychological studies. We also use machine-learning techniques to model the emotion ratings. We create regression models based on the Support Vector Regression algorithm that can estimate the ratings with a correlation of 0.65 in average.},
	language = {en},
	urldate = {2023-10-23},
	journal = {Frontiers in Human Neuroscience},
	author = {Petri, Toiviaine},
	year = {2009},
	file = {Petri - 2009 - Exploring relationships between audio features and.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\XZUCYK5M\\Petri - 2009 - Exploring relationships between audio features and.pdf:application/pdf},
}

@article{kelly_exploring_2021-1,
	title = {Exploring {Changes} in the {Emotional} {Classification} of {Music} between {Eras}},
	volume = {4},
	issn = {2574-2442, 2574-2450},
	url = {https://www.tandfonline.com/doi/full/10.1080/25742442.2021.1988422},
	doi = {10.1080/25742442.2021.1988422},
	language = {en},
	number = {1-2},
	urldate = {2023-10-23},
	journal = {Auditory Perception \& Cognition},
	author = {Kelly, Benjamin O and Anderson, Cameron J and Schutz, Michael},
	month = apr,
	year = {2021},
	pages = {121--131},
	file = {Kelly et al. - 2021 - Exploring Changes in the Emotional Classification .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YEJD5C2P\\Kelly et al. - 2021 - Exploring Changes in the Emotional Classification .pdf:application/pdf},
}

@article{gomez_tonal_2006-1,
	title = {Tonal {Description} of {Polyphonic} {Audio} for {Music} {Content} {Processing}},
	volume = {18},
	issn = {1091-9856, 1526-5528},
	url = {https://pubsonline.informs.org/doi/10.1287/ijoc.1040.0126},
	doi = {10.1287/ijoc.1040.0126},
	abstract = {We present a method to extract a description of the tonal aspects of music from polyphonic audio signals. We define this tonal description using different levels of abstraction, differentiating between low-level signal descriptors and high-level textual labels. We also establish different temporal scales for description, defining some features as being attached to a certain time instant, and other global descriptors as related to a wider segment. The description is validated by estimating the key of a piece. We also propose the description as a tonal representation of the polyphonic audio signal to measure tonal similarity between audio excerpts and to establish the tonal structure of a musical piece.},
	language = {en},
	number = {3},
	urldate = {2023-10-23},
	journal = {INFORMS Journal on Computing},
	author = {Gómez, Emilia},
	month = aug,
	year = {2006},
	pages = {294--304},
	file = {Gómez - 2006 - Tonal Description of Polyphonic Audio for Music Co.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\A9BJ7JWM\\Gómez - 2006 - Tonal Description of Polyphonic Audio for Music Co.pdf:application/pdf},
}

@article{cuthbert_music21_2010-1,
	title = {music21: {A} {Toolkit} for {Computer}-{Aided} {Musicology} and {Symbolic} {Music} {Data}},
	abstract = {Music21 is an object-oriented toolkit for analyzing, searching, and transforming music in symbolic (scorebased) forms. The modular approach of the project allows musicians and researchers to write simple scripts rapidly and reuse them in other projects. The toolkit aims to provide powerful software tools integrated with sophisticated musical knowledge to both musicians with little programming experience (especially musicologists) and to programmers with only modest music theory skills.},
	language = {en},
	author = {Cuthbert, Michael Scott and Ariza, Christopher},
	year = {2010},
	file = {Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\GBCSB6MI\\Cuthbert and Ariza - 2010 - music21 A Toolkit for Computer-Aided Musicology a.pdf:application/pdf},
}

@article{battcock_individualized_2021-1,
	title = {Individualized interpretation: {Exploring} structural and interpretive effects on evaluations of emotional content in {Bach}’s {Well} {Tempered} {Clavier}},
	volume = {50},
	issn = {0929-8215, 1744-5027},
	shorttitle = {Individualized interpretation},
	url = {https://www.tandfonline.com/doi/full/10.1080/09298215.2021.1979050},
	doi = {10.1080/09298215.2021.1979050},
	abstract = {Audiences, juries, and critics continually evaluate performers based on their interpretations of familiar classics. Yet formally assessing the perceptual consequences of interpretive decisions is challenging – particularly with respect to how they shape emotional messages. Here, we explore the issue through comparison of emotion ratings (using scales of arousal and valence) for excerpts of all 48 pieces from Bach’s Well-Tempered Clavier. In this series of studies, participants evaluated one of seven interpretations by highly regarded pianists. This work offers the novel ability to simultaneously explore (1) how different interpretations by expert pianists shape emotional messages, (2) the degree to which structural and interpretative elements shape the clarity of emotional messages, and (3) how interpretative differences affect the strength of specific features or cues to convey musical emotion.},
	language = {en},
	number = {5},
	urldate = {2023-10-23},
	journal = {Journal of New Music Research},
	author = {Battcock, Aimee and Schutz, Michael},
	month = oct,
	year = {2021},
	pages = {447--468},
	file = {Battcock and Schutz - 2021 - Individualized interpretation Exploring structura.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\UTMKRQZK\\Battcock and Schutz - 2021 - Individualized interpretation Exploring structura.pdf:application/pdf},
}

@article{battcock_emotion_2022-1,
	title = {Emotion and expertise: how listeners with formal music training use cues to perceive emotion},
	volume = {86},
	issn = {0340-0727, 1430-2772},
	shorttitle = {Emotion and expertise},
	url = {https://link.springer.com/10.1007/s00426-020-01467-1},
	doi = {10.1007/s00426-020-01467-1},
	abstract = {Although studies of musical emotion often focus on the role of the composer and performer, the communicative process is also influenced by the listener’s musical background or experience. Given the equivocal nature of evidence regarding the effects of musical training, the role of listener expertise in conveyed musical emotion remains opaque. Here we examine emotional responses of musically trained listeners across two experiments using (1) eight measure excerpts, (2) musically resolved excerpts and compare them to responses collected from untrained listeners in Battcock and Schutz (2019). In each experiment 30 participants with six or more years of music training rated perceived emotion for 48 excerpts from Bach’s WellTempered Clavier (WTC) using scales of valence and arousal. Models of listener ratings predict more variance in trained vs. untrained listeners across both experiments. More importantly however, we observe a shift in cue weights related to training. Using commonality analysis and Fischer Z score comparisons as well as margin of error calculations, we show that timing and mode affect untrained listeners equally, whereas mode plays a significantly stronger role than timing for trained listeners. This is not to say the emotional messages are less well recognized by untrained listeners—simply that training appears to shift the relative weight of cues used in making evaluations. These results clarify music training’s potential impact on the specific effects of cues in conveying musical emotion.},
	language = {en},
	number = {1},
	urldate = {2023-10-23},
	journal = {Psychological Research},
	author = {Battcock, Aimee and Schutz, Michael},
	month = feb,
	year = {2022},
	pages = {66--86},
	file = {Battcock and Schutz - 2022 - Emotion and expertise how listeners with formal m.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\GA5CA3G8\\Battcock and Schutz - 2022 - Emotion and expertise how listeners with formal m.pdf:application/pdf},
}

@article{battcock_acoustically_2019-1,
	title = {Acoustically {Expressing} {Affect}},
	volume = {37},
	issn = {0730-7829, 1533-8312},
	url = {https://online.ucpress.edu/mp/article/37/1/66/62853/Acoustically-Expressing-Affect},
	doi = {10.1525/mp.2019.37.1.66},
	abstract = {Composers convey emotion through music by co-varying structural cues. Although the complex interplay provides a rich listening experience, this creates challenges for understanding the contributions of individual cues. Here we investigate how three specific cues (attack rate, mode, and pitch height) work together to convey emotion in Bach's Well Tempered-Clavier (WTC). In three experiments, we explore responses to (1) eight-measure excerpts and (2) musically “resolved” excerpts, and (3) investigate the role of different standard dimensional scales of emotion. In each experiment, thirty nonmusician participants rated perceived emotion along scales of valence and intensity (Experiments 1 \& 2) or valence and arousal (Experiment 3) for 48 pieces in the WTC. Responses indicate listeners used attack rate, Mode, and pitch height to make judgements of valence, but only attack rate for intensity/arousal. Commonality analyses revealed mode predicted the most variance for valence ratings, followed by attack rate, with pitch height contributing minimally. In Experiment 2 mode increased in predictive power compared to Experiment 1. For Experiment 3, using “arousal” instead of “intensity” showed similar results to Experiment 1. We discuss how these results complement and extend previous findings of studies with tightly controlled stimuli, providing additional perspective on complex issues of interpersonal communication.},
	language = {en},
	number = {1},
	urldate = {2023-10-23},
	journal = {Music Perception},
	author = {Battcock, Aimee and Schutz, Michael},
	month = sep,
	year = {2019},
	pages = {66--91},
	file = {Battcock and Schutz - 2019 - Acoustically Expressing Affect.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\LGQGTI9K\\Battcock and Schutz - 2019 - Acoustically Expressing Affect.pdf:application/pdf},
}

@article{anderson_exploring_2022-1,
	title = {Exploring historic changes in musical communication: {Deconstructing} emotional cues in preludes by {Bach} and {Chopin}},
	volume = {50},
	issn = {0305-7356, 1741-3087},
	shorttitle = {Exploring historic changes in musical communication},
	url = {http://journals.sagepub.com/doi/10.1177/03057356211046375},
	doi = {10.1177/03057356211046375},
	abstract = {A growing body of research analyzing musical scores suggests mode’s relationship with other expressive cues has changed over time. However, to the best of our knowledge, the perceptual implications of these changes have not been formally assessed. Here, we explore how compositional choices of 17th- and 19th-century composers (J. S. Bach and F. Chopin, respectively) differentially affect emotional communication. This novel exploration builds on our team’s previous techniques using commonality analysis to decompose intercorrelated cues in unaltered excerpts of influential compositions. In doing so, we offer an important naturalistic complement to traditional experimental work—often involving tightly controlled stimuli constructed to avoid the intercorrelations inherent to naturalistic music. Our data indicate intriguing changes in cues’ effects between Bach and Chopin, consistent with score-based research suggesting mode’s “meaning” changed across historical eras. For example, mode’s unique effect accounts for the most variance in valence ratings of Chopin’s preludes, whereas its shared use with attack rate plays a more prominent role in Bach’s. We discuss the implications of these findings as part of our field’s ongoing effort to understand the complexity of musical communication—addressing issues only visible when moving beyond stimuli created for scientific, rather than artistic, goals.},
	language = {en},
	number = {5},
	urldate = {2023-10-23},
	journal = {Psychology of Music},
	author = {Anderson, Cameron J. and Schutz, Michael},
	month = sep,
	year = {2022},
	pages = {1424--1442},
	file = {Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\67T3JSLK\\Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:application/pdf},
}

@article{anderson_exploring_2022-2,
	title = {Exploring historic changes in musical communication: {Deconstructing} emotional cues in preludes by {Bach} and {Chopin}},
	volume = {50},
	issn = {0305-7356, 1741-3087},
	shorttitle = {Exploring historic changes in musical communication},
	url = {http://journals.sagepub.com/doi/10.1177/03057356211046375},
	doi = {10.1177/03057356211046375},
	abstract = {A growing body of research analyzing musical scores suggests mode’s relationship with other expressive cues has changed over time. However, to the best of our knowledge, the perceptual implications of these changes have not been formally assessed. Here, we explore how compositional choices of 17th- and 19th-century composers (J. S. Bach and F. Chopin, respectively) differentially affect emotional communication. This novel exploration builds on our team’s previous techniques using commonality analysis to decompose intercorrelated cues in unaltered excerpts of influential compositions. In doing so, we offer an important naturalistic complement to traditional experimental work—often involving tightly controlled stimuli constructed to avoid the intercorrelations inherent to naturalistic music. Our data indicate intriguing changes in cues’ effects between Bach and Chopin, consistent with score-based research suggesting mode’s “meaning” changed across historical eras. For example, mode’s unique effect accounts for the most variance in valence ratings of Chopin’s preludes, whereas its shared use with attack rate plays a more prominent role in Bach’s. We discuss the implications of these findings as part of our field’s ongoing effort to understand the complexity of musical communication—addressing issues only visible when moving beyond stimuli created for scientific, rather than artistic, goals.},
	language = {en},
	number = {5},
	urldate = {2023-10-23},
	journal = {Psychology of Music},
	author = {Anderson, Cameron J. and Schutz, Michael},
	month = sep,
	year = {2022},
	pages = {1424--1442},
	file = {Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\QE2Z2G67\\Anderson and Schutz - 2022 - Exploring historic changes in musical communicatio.pdf:application/pdf},
}

@misc{yesiler_less_2020-1,
	title = {Less is more: {Faster} and better music version identification with embedding distillation},
	shorttitle = {Less is more},
	url = {http://arxiv.org/abs/2010.03284},
	abstract = {Version identiﬁcation systems aim to detect different renditions of the same underlying musical composition (loosely called cover songs). By learning to encode entire recordings into plain vector embeddings, recent systems have made signiﬁcant progress in bridging the gap between accuracy and scalability, which has been a key challenge for nearly two decades. In this work, we propose to further narrow this gap by employing a set of data distillation techniques that reduce the embedding dimensionality of a pre-trained state-of-the-art model. We compare a wide range of techniques and propose new ones, from classical dimensionality reduction to more sophisticated distillation schemes. With those, we obtain 99\% smaller embeddings that, moreover, yield up to a 3\% accuracy increase. Such small embeddings can have an important impact in retrieval time, up to the point of making a real-world system practical on a standalone laptop.},
	language = {en},
	urldate = {2023-10-23},
	publisher = {arXiv},
	author = {Yesiler, Furkan and Serrà, Joan and Gómez, Emilia},
	month = oct,
	year = {2020},
	note = {arXiv:2010.03284 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Yesiler et al. - 2020 - Less is more Faster and better music version iden.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\MTWKRARW\\Yesiler et al. - 2020 - Less is more Faster and better music version iden.pdf:application/pdf},
}

@article{yang_machine_2012-1,
	title = {Machine {Recognition} of {Music} {Emotion}: {A} {Review}},
	volume = {3},
	issn = {2157-6904, 2157-6912},
	shorttitle = {Machine {Recognition} of {Music} {Emotion}},
	url = {https://dl.acm.org/doi/10.1145/2168752.2168754},
	doi = {10.1145/2168752.2168754},
	abstract = {The proliferation of MP3 players and the exploding amount of digital music content call for novel ways of music organization and retrieval to meet the ever-increasing demand for easy and effective information access. As almost every music piece is created to convey emotion, music organization and retrieval by emotion is a reasonable way of accessing music information. A good deal of effort has been made in the music information retrieval community to train a machine to automatically recognize the emotion of a music signal. A central issue of machine recognition of music emotion is the conceptualization of emotion and the associated emotion taxonomy. Different viewpoints on this issue have led to the proposal of different ways of emotion annotation, model training, and result visualization. This article provides a comprehensive review of the methods that have been proposed for music emotion recognition. Moreover, as music emotion recognition is still in its infancy, there are many open issues. We review the solutions that have been proposed to address these issues and conclude with suggestions for further research.},
	language = {en},
	number = {3},
	urldate = {2023-10-23},
	journal = {ACM Transactions on Intelligent Systems and Technology},
	author = {Yang, Yi-Hsuan and Chen, Homer H.},
	month = may,
	year = {2012},
	pages = {1--30},
	file = {Yang and Chen - 2012 - Machine Recognition of Music Emotion A Review.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\PX3DUHYB\\Yang and Chen - 2012 - Machine Recognition of Music Emotion A Review.pdf:application/pdf},
}

@article{yi-hsuan_yang_ranking-based_2011-1,
	title = {Ranking-{Based} {Emotion} {Recognition} for {Music} {Organization} and {Retrieval}},
	volume = {19},
	issn = {1558-7916, 1558-7924},
	url = {http://ieeexplore.ieee.org/document/5545401/},
	doi = {10.1109/TASL.2010.2064164},
	abstract = {Determining the emotion of a song that best characterizes the affective content of the song is a challenging issue due to the difﬁculty of collecting reliable ground truth data and the semantic gap between human’s perception and the music signal of the song. To address this issue, we represent an emotion as a point in the Cartesian space with valence and arousal as the dimensions and determine the coordinates of a song by the relative emotion of the song with respect to other songs. We also develop an RBF-ListNet algorithm to optimize the ranking-based objective function of our approach. The cognitive load of annotation, the accuracy of emotion recognition, and the subjective quality of the proposed approach are extensively evaluated. Experimental results show that this ranking-based approach simpliﬁes emotion annotation and enhances the reliability of the ground truth. The performance of our algorithm for valence recognition reaches 0.326 in Gamma statistic.},
	language = {en},
	number = {4},
	urldate = {2023-10-23},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {{Yi-Hsuan Yang} and Chen, Homer H},
	month = may,
	year = {2011},
	pages = {762--774},
	file = {Yi-Hsuan Yang and Chen - 2011 - Ranking-Based Emotion Recognition for Music Organi.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\6M3PCTPN\\Yi-Hsuan Yang and Chen - 2011 - Ranking-Based Emotion Recognition for Music Organi.pdf:application/pdf},
}

@article{tzanetakis_pitch_2003-1,
	title = {Pitch {Histograms} in {Audio} and {Symbolic} {Music} {Information} {Retrieval}},
	volume = {32},
	issn = {0929-8215},
	url = {http://www.tandfonline.com/doi/abs/10.1076/jnmr.32.2.143.16743},
	doi = {10.1076/jnmr.32.2.143.16743},
	abstract = {In order to represent musical content, pitch and timing information is utilized in the majority of existing work in Symbolic Music Information Retrieval (MIR). Symbolic representations such as MIDI allow the easy calculation of such information and its manipulation. In contrast, most of the existing work in Audio MIR uses timbral and beat information, which can be calculated using automatic computer audition techniques.},
	language = {en},
	number = {2},
	urldate = {2023-10-23},
	journal = {Journal of New Music Research},
	author = {Tzanetakis, George and Ermolinskyi, Andrey and Cook, Perry},
	month = jun,
	year = {2003},
	pages = {143--152},
	file = {Tzanetakis et al. - 2003 - Pitch Histograms in Audio and Symbolic Music Infor.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\A6ED23IZ\\Tzanetakis et al. - 2003 - Pitch Histograms in Audio and Symbolic Music Infor.pdf:application/pdf},
}

@inproceedings{wang_analysis_2013-1,
	address = {Chengdu, China},
	title = {The {Analysis} and {Comparison} of {Vital} {Acoustic} {Features} in {Content}-{Based} {Classification} of {Music} {Genre}},
	isbn = {978-1-4799-2877-4 978-1-4799-2876-7},
	url = {http://ieeexplore.ieee.org/document/6710015/},
	doi = {10.1109/ITA.2013.99},
	abstract = {Digital music is becoming increasingly popular in the Internet, and content-based musical genre classification has gained significant attentions in the field of musical retrieval. In this paper, the acoustic musical features are extracted from the viewpoints of both signal processing and the musical dimension. By comparing the performance of classifier of different combination of acoustic features, the contributions of corresponding features are evaluated. Finally, timbre and tonality feature sets are found to be the most effective features in music genre recognition.},
	language = {en},
	urldate = {2023-10-23},
	booktitle = {2013 {International} {Conference} on {Information} {Technology} and {Applications}},
	publisher = {IEEE},
	author = {Wang, Zhe and Xia, Jingbo and Luo, Bin},
	month = nov,
	year = {2013},
	pages = {404--408},
	file = {Wang et al. - 2013 - The Analysis and Comparison of Vital Acoustic Feat.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\PCQ7K5RH\\Wang et al. - 2013 - The Analysis and Comparison of Vital Acoustic Feat.pdf:application/pdf},
}

@incollection{kacprzyk_audio_2010-1,
	address = {Berlin, Heidelberg},
	title = {Audio {Cover} {Song} {Identification} and {Similarity}: {Background}, {Approaches}, {Evaluation}, and {Beyond}},
	volume = {274},
	isbn = {978-3-642-11673-5 978-3-642-11674-2},
	shorttitle = {Audio {Cover} {Song} {Identification} and {Similarity}},
	url = {https://link.springer.com/10.1007/978-3-642-11674-2_14},
	language = {en},
	urldate = {2023-10-23},
	booktitle = {Advances in {Music} {Information} {Retrieval}},
	publisher = {Springer Berlin Heidelberg},
	author = {Serrà, Joan and Gómez, Emilia and Herrera, Perfecto},
	editor = {Kacprzyk, Janusz and Raś, Zbigniew W. and Wieczorkowska, Alicja A.},
	year = {2010},
	doi = {10.1007/978-3-642-11674-2_14},
	note = {Series Title: Studies in Computational Intelligence},
	pages = {307--332},
	file = {Serrà et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\93I7K6MH\\Serrà et al. - 2010 - Audio Cover Song Identification and Similarity Ba.pdf:application/pdf},
}

@article{urbano_what_2014,
	title = {{WHAT} {IS} {THE} {EFFECT} {OF} {AUDIO} {QUALITY} {ON} {THE} {ROBUSTNESS} {OF} {MFCCs} {AND} {CHROMA} {FEATURES}?},
	abstract = {Music Information Retrieval is largely based on descriptors computed from audio signals, and in many practical applications they are to be computed on music corpora containing audio ﬁles encoded in a variety of lossy formats. Such encodings distort the original signal and therefore may affect the computation of descriptors. This raises the question of the robustness of these descriptors across various audio encodings. We examine this assumption for the case of MFCCs and chroma features. In particular, we analyze their robustness to sampling rate, codec, bitrate, frame size and music genre. Using two different audio analysis tools over a diverse collection of music tracks, we compute several statistics to quantify the robustness of the resulting descriptors, and then estimate the practical effects for a sample task like genre classiﬁcation.},
	language = {en},
	author = {Urbano, Julian and Bogdanov, Dmitry and Herrera, Perfecto and Gomez, Emilia and Serra, Xavier},
	year = {2014},
	file = {Urbano et al. - 2014 - WHAT IS THE EFFECT OF AUDIO QUALITY ON THE ROBUSTN.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\SX7ZM92Y\\Urbano et al. - 2014 - WHAT IS THE EFFECT OF AUDIO QUALITY ON THE ROBUSTN.pdf:application/pdf},
}

@article{urbano_evaluation_2013,
	title = {Evaluation in {Music} {Information} {Retrieval}},
	volume = {41},
	issn = {0925-9902, 1573-7675},
	url = {http://link.springer.com/10.1007/s10844-013-0249-4},
	doi = {10.1007/s10844-013-0249-4},
	abstract = {The field of Music Information Retrieval has always acknowledged the need for rigorous scientific evaluations, and several efforts have set out to develop and provide the infrastructure, technology and methodologies needed to carry out these evaluations. The community has enormously gained from these evaluation forums, but we have reached a point where we are stuck with evaluation frameworks that do not allow us to improve as much and as well as we want. The community recently acknowledged this problem and showed interest in addressing it, though it is not clear what to do to improve the situation. We argue that a good place to start is again the Text IR field. Based on a formalization of the evaluation process, this paper presents a survey of past evaluation work in the context of Text IR, from the point of view of validity, reliability and efficiency of the experiments. We show the problems that our community currently has in terms of evaluation, point to several lines of research to improve it and make various proposals in that line.},
	language = {en},
	number = {3},
	urldate = {2023-10-23},
	journal = {Journal of Intelligent Information Systems},
	author = {Urbano, Julián and Schedl, Markus and Serra, Xavier},
	month = dec,
	year = {2013},
	pages = {345--369},
	file = {Urbano et al. - 2013 - Evaluation in Music Information Retrieval.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\USPQD2ZM\\Urbano et al. - 2013 - Evaluation in Music Information Retrieval.pdf:application/pdf},
}

@article{sturm_state_2014,
	title = {The {State} of the {Art} {Ten} {Years} {After} a {State} of the {Art}: {Future} {Research} in {Music} {Information} {Retrieval}},
	volume = {43},
	issn = {0929-8215, 1744-5027},
	shorttitle = {The {State} of the {Art} {Ten} {Years} {After} a {State} of the {Art}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/09298215.2014.894533},
	doi = {10.1080/09298215.2014.894533},
	abstract = {A decade has passed since the ﬁrst review of research on a ‘ﬂagship application’ of music information retrieval (MIR): the problem of music genre recognition (MGR). During this time, about 500 works addressing MGR have been published, and at least 10 campaigns have been run to evaluate MGR systems. This makes MGR one of the most researched areas of MIR. So, where does MGR now lie? We show that in spite of this massive amount of work, MGR does not lie far from where it began, and the paramount reason for this is that most evaluation in MGR lacks validity. We perform a case study of all published research using the most-used benchmark dataset in MGR during the past decade: GTZAN. We show that none of the evaluations in these many works is valid to produce conclusions with respect to recognizing genre, i.e. that a system is using criteria relevant for recognizing genre. In fact, the problems of validity in evaluation also affect research in music emotion recognition and autotagging. We conclude by discussing the implications of our work for MGR and MIR in the next ten years.},
	language = {en},
	number = {2},
	urldate = {2023-10-23},
	journal = {Journal of New Music Research},
	author = {Sturm, Bob L.},
	month = apr,
	year = {2014},
	pages = {147--172},
	file = {Sturm - 2014 - The State of the Art Ten Years After a State of th.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\ICW4WIAT\\Sturm - 2014 - The State of the Art Ten Years After a State of th.pdf:application/pdf},
}

@article{song_evaluation_2012,
	title = {{EVALUATION} {OF} {MUSICAL} {FEATURES} {FOR} {EMOTION} {CLASSIFICATION}},
	abstract = {Because music conveys and evokes feelings, a wealth of research has been performed on music emotion recognition. Previous research has shown that musical mood is linked to features based on rhythm, timbre, spectrum and lyrics. For example, sad music correlates with slow tempo, while happy music is generally faster. However, only limited success has been obtained in learning automatic classiﬁers of emotion in music. In this paper, we collect a ground truth data set of 2904 songs that have been tagged with one of the four words “happy”, “sad”, “angry” and “relaxed”, on the Last.FM web site. An excerpt of the audio is then retrieved from 7Digital.com, and various sets of audio features are extracted using standard algorithms. Two classiﬁers are trained using support vector machines with the polynomial and radial basis function kernels, and these are tested with 10-fold cross validation. Our results show that spectral features outperform those based on rhythm, dynamics, and, to a lesser extent, harmony. We also ﬁnd that the polynomial kernel gives better results than the radial basis function, and that the fusion of different feature sets does not always lead to improved classiﬁcation.},
	language = {en},
	author = {Song, Yading and Dixon, Simon and Pearce, Marcus},
	year = {2012},
	file = {Song et al. - 2012 - EVALUATION OF MUSICAL FEATURES FOR EMOTION CLASSIF.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\ZH7FDLEW\\Song et al. - 2012 - EVALUATION OF MUSICAL FEATURES FOR EMOTION CLASSIF.pdf:application/pdf},
}

@article{cunningham_impact_2012,
	title = {{THE} {IMPACT} {OF} {MIREX} {ON} {SCHOLARLY} {RESEARCH} (2005 – 2010)},
	abstract = {This paper explores the impact of the MIREX (Music Information Retrieval Evaluation eXchange) evaluation initiative on scholarly research. Impact is assessed through a bibliometric evaluation of both the MIREX extended abstracts and the papers citing the MIREX results, the trial framework and methodology, or MIREX datasets. Impact is examined through number of publications and citation analysis. We further explore the primary publication venues for MIREX results, the geographic distribution of both MIREX contributors and researchers citing MIREX results, and the spread of MIREX-based research beyond the MIREX contributor teams. This analysis indicates that research in this area is highly collaborative, has achieved an international dissemination, and has grown to have a significant profile in the research literature.},
	language = {en},
	author = {Cunningham, Sally Jo and Bainbridge, David and Downie, J Stephen},
	year = {2012},
	file = {Cunningham et al. - 2012 - THE IMPACT OF MIREX ON SCHOLARLY RESEARCH (2005 – .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\INQPIZ7M\\Cunningham et al. - 2012 - THE IMPACT OF MIREX ON SCHOLARLY RESEARCH (2005 – .pdf:application/pdf},
}

@book{ras_advances_2010,
	address = {Berlin, Heidelberg},
	series = {Studies in {Computational} {Intelligence}},
	title = {Advances in {Music} {Information} {Retrieval}},
	volume = {274},
	isbn = {978-3-642-11673-5 978-3-642-11674-2},
	url = {https://link.springer.com/10.1007/978-3-642-11674-2},
	language = {en},
	urldate = {2023-10-23},
	publisher = {Springer Berlin Heidelberg},
	editor = {Raś, Zbigniew W. and Wieczorkowska, Alicja A. and Kacprzyk, Janusz},
	year = {2010},
	doi = {10.1007/978-3-642-11674-2},
	file = {Raś and Wieczorkowska - 2010 - Advances in Music Information Retrieval.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\8FFK7E78\\Raś and Wieczorkowska - 2010 - Advances in Music Information Retrieval.pdf:application/pdf},
}

@article{noauthor_scientific_2023,
	title = {The {Scientific} {Evaluation} of {Music} {Information} {Retrieval} {Systems}: {Foundations} and {Future}},
	language = {en},
	year = {2023},
	file = {2023 - The Scientific Evaluation of Music Information Ret.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YFBTVKA6\\2023 - The Scientific Evaluation of Music Information Ret.pdf:application/pdf},
}

@article{vatolkin_robustness_2018,
	title = {Robustness of {Features} and {Classification} {Models} on {Degraded} {Data} {Sets} in {Music} {Classification}},
	copyright = {Closed Access, Creative Commons Namensnennung – Weitergabe unter gleichen Bedingungen 4.0 International},
	url = {https://publikationen.bibliothek.kit.edu/1000127853},
	doi = {10.5445/KSP/1000087327/22},
	abstract = {There exists a large number of supervised music classiﬁcation tasks: Recognition of music genres and emotions, playing instruments, harmonic and melodic properties, temporal and rhythmic characteristics, etc. In recent years, many studies were published in that ﬁeld, which are either focused on complex feature engineering or application and tuning of classiﬁcation algorithms. However, less work is done on the evaluation of model robustness, and music data sets are often limited to music with some common characteristics, so that the question about the generalisation ability of proposed models usually remains unanswered. In this study, we examine and compare the classiﬁcation performance of audio features and classiﬁcation models when applied for recognition of genres and instruments on music data sets which were degraded by means of techniques available in the Audio Degradation Toolbox including attenuation, compression, live and vinyl recording degradations, and addition of noise.},
	language = {en},
	urldate = {2023-11-01},
	author = {Vatolkin, Igor},
	year = {2018},
	note = {Publisher: Karlsruher Institut für Technologie (KIT)},
	file = {Vatolkin - 2018 - Robustness of Features and Classification Models o.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\IRGVZ7Q7\\Vatolkin - 2018 - Robustness of Features and Classification Models o.pdf:application/pdf},
}

@misc{prinz_end--end_2020,
	title = {End-to-{End} {Adversarial} {White} {Box} {Attacks} on {Music} {Instrument} {Classification}},
	url = {http://arxiv.org/abs/2007.14714},
	abstract = {Small adversarial perturbations of input data are able to drastically change performance of machine learning systems, thereby challenging the validity of such systems. We present the very ﬁrst end-to-end adversarial attacks on a music instrument classiﬁcation system allowing to add perturbations directly to audio waveforms instead of spectrograms. Our attacks are able to reduce the accuracy close to a random baseline while at the same time keeping perturbations almost imperceptible and producing misclassiﬁcations to any desired instrument.},
	language = {en},
	urldate = {2023-11-01},
	publisher = {arXiv},
	author = {Prinz, Katharina and Flexer, Arthur},
	month = jul,
	year = {2020},
	note = {arXiv:2007.14714 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning},
	file = {Prinz and Flexer - 2020 - End-to-End Adversarial White Box Attacks on Music .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\FRF789IA\\Prinz and Flexer - 2020 - End-to-End Adversarial White Box Attacks on Music .pdf:application/pdf},
}

@inproceedings{wiggins_semantic_2009,
	address = {San Diego, California, USA},
	title = {Semantic {Gap}?? {Schemantic} {Schmap}!! {Methodological} {Considerations} in the {Scientific} {Study} of {Music}},
	isbn = {978-1-4244-5231-6},
	shorttitle = {Semantic {Gap}?},
	url = {http://ieeexplore.ieee.org/document/5363140/},
	doi = {10.1109/ISM.2009.36},
	abstract = {We argue that it is time to re-evaluate the MIR community’s approach to building artiﬁcial systems which operate over music. We suggest that it is fundamentally problematic to view music simply as data representing audio signals, and that the notion of the so-called “semantic gap” is misleading. We propose a philosophical framework within which scientiﬁc and/or technological study of music can be carried out, free from such artiﬁcial constructions. Ultimately, we argue that Music (as opposed to sound) can be studied only in a context which explicitly allows for, and is built on, (albeit de facto) models of human perception; to do otherwise is not to study Music at all.},
	language = {en},
	urldate = {2023-11-07},
	booktitle = {2009 11th {IEEE} {International} {Symposium} on {Multimedia}},
	publisher = {IEEE},
	author = {Wiggins, Geraint A.},
	year = {2009},
	pages = {477--482},
	file = {Wiggins - 2009 - Semantic Gap Schemantic Schmap!! Methodological .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\FSFELFJP\\Wiggins - 2009 - Semantic Gap Schemantic Schmap!! Methodological .pdf:application/pdf},
}

@misc{mishra_survey_2023,
	title = {A {Survey} on the {Robustness} of {Feature} {Importance} and {Counterfactual} {Explanations}},
	url = {http://arxiv.org/abs/2111.00358},
	abstract = {There exist several methods that aim to address the crucial task of understanding the behaviour of AI/ML models. Arguably, the most popular among them are local explanations that focus on investigating model behaviour for individual instances. Several methods have been proposed for local analysis, but relatively lesser eﬀort has gone into understanding if the explanations are robust and accurately reﬂect the behaviour of underlying models. In this work, we present a survey of the works that analysed the robustness of two classes of local explanations (feature importance and counterfactual explanations) that are popularly used in analysing AI/ML models in ﬁnance. The survey aims to unify existing deﬁnitions of robustness, introduces a taxonomy to classify diﬀerent robustness approaches, and discusses some interesting results. Finally, the survey introduces some pointers about extending current robustness analysis approaches so as to identify reliable explainability methods.},
	language = {en},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Mishra, Saumitra and Dutta, Sanghamitra and Long, Jason and Magazzeni, Daniele},
	month = jan,
	year = {2023},
	note = {arXiv:2111.00358 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Mishra et al. - 2023 - A Survey on the Robustness of Feature Importance a.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\Y36SCNZN\\Mishra et al. - 2023 - A Survey on the Robustness of Feature Importance a.pdf:application/pdf},
}

@misc{turian_im_2020,
	title = {I'm {Sorry} for {Your} {Loss}: {Spectrally}-{Based} {Audio} {Distances} {Are} {Bad} at {Pitch}},
	shorttitle = {I'm {Sorry} for {Your} {Loss}},
	url = {http://arxiv.org/abs/2012.04572},
	abstract = {Growing research demonstrates that synthetic failure modes imply poor generalization. We compare commonly used audio-to-audio losses on a synthetic benchmark, measuring the pitch distance between two stationary sinusoids. The results are surprising: many have poor sense of pitch direction. These shortcomings are exposed using simple rank assumptions. Our task is trivial for humans but difﬁcult for these audio distances, suggesting signiﬁcant progress can be made in self-supervised audio learning by improving current losses.},
	language = {en},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Turian, Joseph and Henry, Max},
	month = dec,
	year = {2020},
	note = {arXiv:2012.04572 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
	file = {Turian and Henry - 2020 - I'm Sorry for Your Loss Spectrally-Based Audio Di.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\9CN6L2X9\\Turian and Henry - 2020 - I'm Sorry for Your Loss Spectrally-Based Audio Di.pdf:application/pdf},
}

@article{vad_design_nodate,
	title = {Design and evaluation of a probabilistic music projection interface},
	abstract = {We describe the design and evaluation of a probabilistic interface for music exploration and casual playlist generation. Predicted subjective features, such as mood and genre, inferred from low-level audio features create a 34dimensional feature space. We use a nonlinear dimensionality reduction algorithm to create 2D music maps of tracks, and augment these with visualisations of probabilistic mappings of selected features and their uncertainty.},
	language = {en},
	author = {Vad, Beatrix and Boland, Daniel and Williamson, John and Murray-Smith, Roderick and Steffensen, Peter Berg},
	file = {Vad et al. - Design and evaluation of a probabilistic music pro.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\3T5TWYBJ\\Vad et al. - Design and evaluation of a probabilistic music pro.pdf:application/pdf},
}

@article{sturm_revisiting_nodate,
	title = {{REVISITING} {PRIORITIES}: {IMPROVING} {MIR} {EVALUATION} {PRACTICES}},
	abstract = {While there is a consensus that evaluation practices in music informatics (MIR) must be improved, there is no consensus about what should be prioritised in order to do so. Priorities include: 1) improving data; 2) improving ﬁgures of merit; 3) employing formal statistical testing; 4) employing cross-validation; and/or 5) implementing transparent, central and immediate evaluation. In this position paper, I argue how these priorities treat only the symptoms of the problem and not its cause: MIR lacks a formal evaluation framework relevant to its aims. I argue that the principal priority is to adapt and integrate the formal design of experiments (DOE) into the MIR research pipeline. Since the aim of DOE is to help one produce the most reliable evidence at the least cost, it stands to reason that DOE will make a signiﬁcant contribution to MIR. Accomplishing this, however, will not be easy, and will require far more effort than is currently being devoted to it.},
	language = {en},
	author = {Sturm, Bob L},
	file = {Sturm - REVISITING PRIORITIES IMPROVING MIR EVALUATION PR.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\M6XX8J5B\\Sturm - REVISITING PRIORITIES IMPROVING MIR EVALUATION PR.pdf:application/pdf},
}

@article{elowsson_modeling_nodate,
	title = {{MODELING} {MUSIC} {MODALITY} {WITH} {A} {KEY}-{CLASS} {INVARIANT} {PITCH} {CHROMA} {CNN}},
	abstract = {This paper presents a convolutional neural network (CNN) that uses input from a polyphonic pitch estimation system to predict perceived minor/major modality in music audio. The pitch activation input is structured to allow the first CNN layer to compute two pitch chromas focused on different octaves. The following layers perform harmony analysis across chroma and time scales. Through max pooling across pitch, the CNN becomes invariant with regards to the key class (i.e., key disregarding mode) of the music. A multilayer perceptron combines the modality activation output with spectral features for the final prediction. The study uses a dataset of 203 excerpts rated by around 20 listeners each, a small challenging data size requiring a carefully designed parameter sharing. With an R2 of about 0.71, the system clearly outperforms previous systems as well as individual human listeners. A final ablation study highlights the importance of using pitch activations processed across longer time scales, and using pooling to facilitate invariance with regards to the key class.},
	language = {en},
	author = {Elowsson, Anders and Friberg, Anders},
	file = {Elowsson and Friberg - MODELING MUSIC MODALITY WITH A KEY-CLASS INVARIANT.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\8HNMDQ3B\\Elowsson and Friberg - MODELING MUSIC MODALITY WITH A KEY-CLASS INVARIANT.pdf:application/pdf},
}

@misc{halmosi_evaluating_2023,
	title = {On {Evaluating} the {Adversarial} {Robustness} of {Semantic} {Segmentation} {Models}},
	url = {http://arxiv.org/abs/2306.14217},
	abstract = {Achieving robustness against adversarial input perturbation is an important and intriguing problem in machine learning. In the area of semantic image segmentation, a number of adversarial training approaches have been proposed as a defense against adversarial perturbation, but the methodology of evaluating the robustness of the models is still lacking, compared to image classification. Here, we demonstrate that, just like in image classification, it is important to evaluate the models over several different and hard attacks. We propose a set of gradient based iterative attacks and show that it is essential to perform a large number of iterations. We include attacks against the internal representations of the models as well. We apply two types of attacks: maximizing the error with a bounded perturbation, and minimizing the perturbation for a given level of error. Using this set of attacks, we show for the first time that a number of models in previous work that are claimed to be robust are in fact not robust at all. We then evaluate simple adversarial training algorithms that produce reasonably robust models even under our set of strong attacks. Our results indicate that a key design decision to achieve any robustness is to use only adversarial examples during training. However, this introduces a trade-off between robustness and accuracy.},
	language = {en},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Halmosi, Levente and Jelasity, Mark},
	month = jun,
	year = {2023},
	note = {arXiv:2306.14217 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Halmosi and Jelasity - 2023 - On Evaluating the Adversarial Robustness of Semant.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\DWPYKBB7\\Halmosi and Jelasity - 2023 - On Evaluating the Adversarial Robustness of Semant.pdf:application/pdf},
}

@misc{sturm_validity_2023,
	title = {Validity in {Music} {Information} {Research} {Experiments}},
	url = {http://arxiv.org/abs/2301.01578},
	abstract = {Validity is the truth of an inference made from evidence, such as data collected in an experiment, and is central to working scientiﬁcally. Given the maturity of the domain of music information research (MIR), validity in our opinion should be discussed and considered much more than it has been so far. Considering validity in one’s work can improve its scientiﬁc and engineering value. Puzzling MIR phenomena like adversarial attacks and performance glass ceilings become less mysterious through the lens of validity. In this article, we review the subject of validity in general, considering the four major types of validity from a key reference: Shadish et al. [2002]. We ground our discussion of these types with a prototypical MIR experiment: music classiﬁcation using machine learning. Through this MIR experimentalists can be guided to make valid inferences from data collected from their experiments.},
	language = {en},
	urldate = {2023-11-22},
	publisher = {arXiv},
	author = {Sturm, Bob L. T. and Flexer, Arthur},
	month = jan,
	year = {2023},
	note = {arXiv:2301.01578 [cs, eess]},
	keywords = {Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing},
	file = {Sturm and Flexer - 2023 - Validity in Music Information Research Experiments.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\SIZU2KQT\\Sturm and Flexer - 2023 - Validity in Music Information Research Experiments.pdf:application/pdf},
}

@article{siedenburg_comparison_2016,
	title = {A {Comparison} of {Approaches} to {Timbre} {Descriptors} in {Music} {Information} {Retrieval} and {Music} {Psychology}},
	volume = {45},
	issn = {0929-8215, 1744-5027},
	url = {http://www.tandfonline.com/doi/full/10.1080/09298215.2015.1132737},
	doi = {10.1080/09298215.2015.1132737},
	abstract = {A curious divide characterizes the usage of audio descriptors for timbre research in music information research (MIR) and music psychology. While MIR uses a multitude of audio descriptors for tasks such as automatic instrument classiﬁcation, only a highly constrained set is used to describe the physical correlates of timbre perception in parts of music psychology. We argue that this gap is not coincidental and results from the differences in the two ﬁelds’ methodologies, their epistemic groundwork, and research goals. This paper lays out perspectives on the emergence of the divide and reviews studies in both ﬁelds with regards to divergences in research methods and goals. We discuss new representations for spectro-temporal modulations in MIR and psychology, and compare approaches to spectral envelope description in depth. Finally, we will propose that the interdisciplinary discourse on the computational modelling of music requires negotiations about the roles of scientiﬁc evaluation criteria.},
	language = {en},
	number = {1},
	urldate = {2023-11-22},
	journal = {Journal of New Music Research},
	author = {Siedenburg, Kai and Fujinaga, Ichiro and McAdams, Stephen},
	month = jan,
	year = {2016},
	pages = {27--41},
	file = {Siedenburg et al. - 2016 - A Comparison of Approaches to Timbre Descriptors i.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\SZGMZ77B\\Siedenburg et al. - 2016 - A Comparison of Approaches to Timbre Descriptors i.pdf:application/pdf},
}

@article{crowder_perception_1984,
	title = {Perception of the major/minor distinction: {I}. {Historical} and theoretical foundations.},
	volume = {4},
	issn = {2162-1535, 0275-3987},
	shorttitle = {Perception of the major/minor distinction},
	url = {https://doi.apa.org/doi/10.1037/h0094207},
	doi = {10.1037/h0094207},
	language = {en},
	number = {1-2},
	urldate = {2024-02-27},
	journal = {Psychomusicology: A Journal of Research in Music Cognition},
	author = {Crowder, Robert G.},
	year = {1984},
	pages = {3--12},
	file = {Crowder - 1984 - Perception of the majorminor distinction I. Hist.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\76KD4BBH\\Crowder - 1984 - Perception of the majorminor distinction I. Hist.pdf:application/pdf},
}

@article{gagnon_mode_2003,
	title = {Mode and tempo relative contributions to “happy-sad” judgements in equitone melodies},
	volume = {17},
	issn = {0269-9931, 1464-0600},
	url = {https://www.tandfonline.com/doi/full/10.1080/02699930302279},
	doi = {10.1080/02699930302279},
	language = {en},
	number = {1},
	urldate = {2024-02-27},
	journal = {Cognition and Emotion},
	author = {Gagnon, Lise and Peretz, Isabelle},
	month = jan,
	year = {2003},
	pages = {25--40},
	file = {Gagnon and Peretz - 2003 - Mode and tempo relative contributions to “happy-sa.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\GL7GH5T6\\Gagnon and Peretz - 2003 - Mode and tempo relative contributions to “happy-sa.pdf:application/pdf},
}

@article{peeters_timbre_2011,
	title = {The {Timbre} {Toolbox}: {Extracting} audio descriptors from musical signals},
	volume = {130},
	issn = {0001-4966, 1520-8524},
	shorttitle = {The {Timbre} {Toolbox}},
	url = {https://pubs.aip.org/jasa/article/130/5/2902/842365/The-Timbre-Toolbox-Extracting-audio-descriptors},
	doi = {10.1121/1.3642604},
	abstract = {The analysis of musical signals to extract audio descriptors that can potentially characterize their timbre has been disparate and often too focused on a particular small set of sounds. The Timbre Toolbox provides a comprehensive set of descriptors that can be useful in perceptual research, as well as in music information retrieval and machine-learning approaches to content-based retrieval in large sound databases. Sound events are first analyzed in terms of various input representations (short-term Fourier transform, harmonic sinusoidal components, an auditory model based on the equivalent rectangular bandwidth concept, the energy envelope). A large number of audio descriptors are then derived from each of these representations to capture temporal, spectral, spectrotemporal, and energetic properties of the sound events. Some descriptors are global, providing a single value for the whole sound event, whereas others are time-varying. Robust descriptive statistics are used to characterize the time-varying descriptors. To examine the information redundancy across audio descriptors, correlational analysis followed by hierarchical clustering is performed. This analysis suggests ten classes of relatively independent audio descriptors, showing that the Timbre Toolbox is a multidimensional instrument for the measurement of the acoustical structure of complex sound signals.},
	language = {en},
	number = {5},
	urldate = {2024-05-14},
	journal = {The Journal of the Acoustical Society of America},
	author = {Peeters, Geoffroy and Giordano, Bruno L. and Susini, Patrick and Misdariis, Nicolas and McAdams, Stephen},
	month = nov,
	year = {2011},
	pages = {2902--2916},
	file = {Full Text:C\:\\Users\\Konrad\\Zotero\\storage\\5QG3DKHV\\Peeters et al. - 2011 - The Timbre Toolbox Extracting audio descriptors f.pdf:application/pdf},
}

@article{huron_music_2002,
	title = {Music {Information} {Processing} {Using} the {Humdrum} {Toolkit}: {Concepts}, {Examples}, and {Lessons}},
	volume = {26},
	issn = {0148-9267, 1531-5169},
	shorttitle = {Music {Information} {Processing} {Using} the {Humdrum} {Toolkit}},
	url = {https://direct.mit.edu/comj/article/26/2/11-26/93738},
	doi = {10.1162/014892602760137158},
	language = {en},
	number = {2},
	urldate = {2024-05-14},
	journal = {Computer Music Journal},
	author = {Huron, David},
	month = jun,
	year = {2002},
	pages = {11--26},
	file = {Huron - 2002 - Music Information Processing Using the Humdrum Too.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\JYSA3F4S\\Huron - 2002 - Music Information Processing Using the Humdrum Too.pdf:application/pdf},
}

@article{mcennis_jaudio_nodate,
	title = {{JAUDIO}: {A} {FEATURE} {EXTRACTION} {LIBRARY}},
	language = {en},
	author = {McEnnis, Daniel and McKay, Cory and Fujinaga, Ichiro and Depalle, Philippe},
	file = {McEnnis et al. - JAUDIO A FEATURE EXTRACTION LIBRARY.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\EZMTLR75\\McEnnis et al. - JAUDIO A FEATURE EXTRACTION LIBRARY.pdf:application/pdf},
}

@article{cataltepe_music_2007,
	title = {Music {Genre} {Classification} {Using} {MIDI} and {Audio} {Features}},
	volume = {2007},
	issn = {1687-6180},
	url = {https://asp-eurasipjournals.springeropen.com/articles/10.1155/2007/36409},
	doi = {10.1155/2007/36409},
	language = {en},
	number = {1},
	urldate = {2024-05-15},
	journal = {EURASIP Journal on Advances in Signal Processing},
	author = {Cataltepe, Zehra and Yaslan, Yusuf and Sonmez, Abdullah},
	month = dec,
	year = {2007},
	pages = {036409},
	file = {Cataltepe et al. - 2007 - Music Genre Classification Using MIDI and Audio Fe.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\SK95YEJT\\Cataltepe et al. - 2007 - Music Genre Classification Using MIDI and Audio Fe.pdf:application/pdf},
}

@article{hu_bootstrap_nodate,
	title = {A {Bootstrap} {Method} for {Training} an {Accurate} {Audio} {Segmenter}},
	abstract = {Supervised learning can be used to create good systems for note segmentation in audio data. However, this requires a large set of labeled training examples, and handlabeling is quite difﬁcult and time consuming. A bootstrap approach is introduced in which audio alignment techniques are ﬁrst used to ﬁnd the correspondence between a symbolic music representation (such as MIDI data) and an acoustic recording. This alignment provides an initial estimate of note boundaries which can be used to train a segmenter. Once trained, the segmenter can be used to reﬁne the initial set of note boundaries and training can be repeated. This iterative training process eliminates the need for hand-segmented audio. Tests show that this training method can improve a segmenter initially trained on synthetic data.},
	language = {en},
	author = {Hu, Ning and Dannenberg, Roger B},
	file = {Hu and Dannenberg - A Bootstrap Method for Training an Accurate Audio .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\CX7DS8WF\\Hu and Dannenberg - A Bootstrap Method for Training an Accurate Audio .pdf:application/pdf},
}

@article{raffel_large-scale_nodate,
	title = {{LARGE}-{SCALE} {CONTENT}-{BASED} {MATCHING} {OF} {MIDI} {AND} {AUDIO} {FILES}},
	abstract = {MIDI ﬁles, when paired with corresponding audio recordings, can be used as ground truth for many music information retrieval tasks. We present a system which can efﬁciently match and align MIDI ﬁles to entries in a large corpus of audio content based solely on content, i.e., without using any metadata. The core of our approach is a convolutional network-based cross-modality hashing scheme which transforms feature matrices into sequences of vectors in a common Hamming space. Once represented in this way, we can efﬁciently perform large-scale dynamic time warping searches to match MIDI data to audio recordings. We evaluate our approach on the task of matching a huge corpus of MIDI ﬁles to the Million Song Dataset.},
	language = {en},
	author = {Raffel, Colin and Ellis, Daniel P W},
	file = {Raffel and Ellis - LARGE-SCALE CONTENT-BASED MATCHING OF MIDI AND AUD.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\5FWLRKEZ\\Raffel and Ellis - LARGE-SCALE CONTENT-BASED MATCHING OF MIDI AND AUD.pdf:application/pdf},
}

@article{mauch_audio_nodate,
	title = {{THE} {AUDIO} {DEGRADATION} {TOOLBOX} {AND} {ITS} {APPLICATION} {TO} {ROBUSTNESS} {EVALUATION}},
	abstract = {We introduce the Audio Degradation Toolbox (ADT) for the controlled degradation of audio signals, and propose its usage as a means of evaluating and comparing the robustness of audio processing algorithms. Music recordings encountered in practical applications are subject to varied, sometimes unpredictable degradation. For example, audio is degraded by low-quality microphones, noisy recording environments, MP3 compression, dynamic compression in broadcasting or vinyl decay. In spite of this, no standard software for the degradation of audio exists, and music processing methods are usually evaluated against clean data. The ADT ﬁlls this gap by providing Matlab scripts that emulate a wide range of degradation types. We describe 14 degradation units, and how they can be chained to create more complex, ‘real-world’ degradations. The ADT also provides functionality to adjust existing ground-truth, correcting for temporal distortions introduced by degradation. Using four different music informatics tasks, we show that performance strongly depends on the combination of method and degradation applied. We demonstrate that speciﬁc degradations can reduce or even reverse the performance difference between two competing methods. ADT source code, sounds, impulse responses and deﬁnitions are freely available for download.},
	language = {en},
	author = {Mauch, Matthias and Ewert, Sebastian},
	file = {Mauch and Ewert - THE AUDIO DEGRADATION TOOLBOX AND ITS APPLICATION .pdf:C\:\\Users\\Konrad\\Zotero\\storage\\PV7USSLQ\\Mauch and Ewert - THE AUDIO DEGRADATION TOOLBOX AND ITS APPLICATION .pdf:application/pdf},
}

@article{lahdelma_is_2022,
	title = {Is {Harmonicity} a {Misnomer} for {Cultural} {Familiarity} in {Consonance} {Preferences}?},
	volume = {13},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2022.802385/full},
	doi = {10.3389/fpsyg.2022.802385},
	language = {en},
	urldate = {2024-05-15},
	journal = {Frontiers in Psychology},
	author = {Lahdelma, Imre and Eerola, Tuomas and Armitage, James},
	month = jan,
	year = {2022},
	pages = {802385},
	file = {Lahdelma et al. - 2022 - Is Harmonicity a Misnomer for Cultural Familiarity.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\6FNU934S\\Lahdelma et al. - 2022 - Is Harmonicity a Misnomer for Cultural Familiarity.pdf:application/pdf},
}

@article{gotham_when_2023,
	title = {When in {Rome}: {A} {Meta}-corpus of {Functional} {Harmony}},
	volume = {6},
	issn = {2514-3298},
	shorttitle = {When in {Rome}},
	url = {https://transactions.ismir.net/articles/10.5334/tismir.165/},
	doi = {10.5334/tismir.165},
	abstract = {When in Rome’ brings together all human-made, computer-encoded, functional harmonic analyses of music. This amounts in total to over 2,000 analyses of 1,500 distinct works. The most obvious motivation is scale: gathering these datasets together leads to a corpus large and varied enough for tasks including machine learning for automatic analysis, composition, and classification, as well as at-scale anthology creation and more. Further benefits include bringing together a range of different composers and genres (previous datasets typically limit themselves to one context), and of analytical perspectives on those works. We offer this data in as readyto-use and reproducible a state as possible at http://github.com/MarkGotham/Whenin-Rome, with code and documentation for all tasks reported here, including corpus conversion routines and feature extraction.},
	language = {en},
	number = {1},
	urldate = {2024-05-15},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Gotham, Mark and Micchi, Gianluca and López, Néstor Nápoles and Sailor, Malcolm},
	month = nov,
	year = {2023},
	pages = {150--166},
	file = {Gotham et al. - 2023 - When in Rome A Meta-corpus of Functional Harmony.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YGZQDEQJ\\Gotham et al. - 2023 - When in Rome A Meta-corpus of Functional Harmony.pdf:application/pdf},
}

@article{chen_attend_2021,
	title = {Attend to {Chords}: {Improving} {Harmonic} {Analysis} of {Symbolic} {Music} {Using} {Transformer}-{Based} {Models}},
	volume = {4},
	issn = {2514-3298},
	shorttitle = {Attend to {Chords}},
	url = {http://transactions.ismir.net/articles/10.5334/tismir.65/},
	doi = {10.5334/tismir.65},
	language = {en},
	number = {1},
	urldate = {2024-05-15},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Chen, Tsung-Ping and Su, Li},
	month = feb,
	year = {2021},
	pages = {1--13},
	file = {Chen and Su - 2021 - Attend to Chords Improving Harmonic Analysis of S.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\9R8T8TH8\\Chen and Su - 2021 - Attend to Chords Improving Harmonic Analysis of S.pdf:application/pdf},
}

@inproceedings{zheng_cover_2023,
	address = {Lingshui China},
	title = {Cover {Song} {Identification} {Technologies}: {A} {Survey}},
	isbn = {9798400708909},
	shorttitle = {Cover {Song} {Identification} {Technologies}},
	url = {https://dl.acm.org/doi/10.1145/3638884.3638891},
	doi = {10.1145/3638884.3638891},
	abstract = {Cover Song Identification (CSI) is a highly crucial task in the field of Music Information Retrieval (MIR) and holds significant importance in the domain of copyright infringement detection. With the widespread adoption of digital music and the online dissemination of musical works, issues related to music copyright violations have become increasingly prevalent and severe. Music copyright holders and creators face the risk of their musical compositions being copied, interpreted, distributed, or pirated without proper authorization, leading to potential misuse of their works and causing financial losses, as well as reputational damage. As a result, cover song identification technology has emerged as a vital tool for safeguarding music copyrights and preserving the legitimate rights of creators. In essence, the significance of cover song identification in the context of copyright infringement detection lies not only in the protection of music copyrights but also in maintaining a healthy ecosystem for music rights. Furthermore, it enhances the user experience on music platforms by preventing unauthorized cover versions from infiltrating recommendation systems.In conclusion, cover song identification technology provides robust support for the sustainable development of the music industry and the protection of creators’ legitimate rights. It plays an indispensable role in ensuring the long-term prosperity of the music sector. This article aims to provide a comprehensive exposition of cover song identification technologies.},
	language = {en},
	urldate = {2024-05-15},
	booktitle = {Proceedings of the 2023 9th {International} {Conference} on {Communication} and {Information} {Processing}},
	publisher = {ACM},
	author = {Zheng, Yang and Liu, Jie and Zhang, Wu Shu},
	month = dec,
	year = {2023},
	pages = {38--42},
	file = {Zheng et al. - 2023 - Cover Song Identification Technologies A Survey.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\NK2C5ANK\\Zheng et al. - 2023 - Cover Song Identification Technologies A Survey.pdf:application/pdf},
}

@book{klapuri_signal_2006,
	address = {New York, NY},
	title = {Signal processing methods for music transcription},
	isbn = {978-0-387-30667-4},
	language = {en},
	publisher = {Springer},
	editor = {Klapuri, Anssi and Davy, Manuel},
	year = {2006},
	file = {Klapuri and Davy - 2006 - Signal processing methods for music transcription.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\UHWAVBB8\\Klapuri and Davy - 2006 - Signal processing methods for music transcription.pdf:application/pdf},
}

@article{eerola_emotional_2013,
	title = {Emotional expression in music: contribution, linearity, and additivity of primary musical cues},
	volume = {4},
	issn = {1664-1078},
	shorttitle = {Emotional expression in music},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00487/abstract},
	doi = {10.3389/fpsyg.2013.00487},
	abstract = {The aim of this study is to manipulate musical cues systematically to determine the aspects of music that contribute to emotional expression, and whether these cues operate in additive or interactive fashion, and whether the cue levels can be characterized as linear or non-linear. An optimized factorial design was used with six primary musical cues (mode, tempo, dynamics, articulation, timbre, and register) across four different music examples. Listeners rated 200 musical examples according to four perceived emotional characters (happy, sad, peaceful, and scary). The results exhibited robust effects for all cues and the ranked importance of these was established by multiple regression. The most important cue was mode followed by tempo, register, dynamics, articulation, and timbre, although the ranking varied across the emotions. The second main result suggested that most cue levels contributed to the emotions in a linear fashion, explaining 77–89\% of variance in ratings. Quadratic encoding of cues did lead to minor but signiﬁcant increases of the models (0–8\%). Finally, the interactions between the cues were non-existent suggesting that the cues operate mostly in an additive fashion, corroborating recent ﬁndings on emotional expression in music (Juslin and Lindström, 2010).},
	language = {en},
	urldate = {2024-05-15},
	journal = {Frontiers in Psychology},
	author = {Eerola, Tuomas and Friberg, Anders and Bresin, Roberto},
	year = {2013},
	file = {Eerola et al. - 2013 - Emotional expression in music contribution, linea.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\X65MNMQU\\Eerola et al. - 2013 - Emotional expression in music contribution, linea.pdf:application/pdf},
}

@article{tzanetakis_musical_2002,
	title = {Musical genre classification of audio signals},
	volume = {10},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {1063-6676, 1558-2353},
	url = {https://ieeexplore.ieee.org/document/1021072/},
	doi = {10.1109/TSA.2002.800560},
	abstract = {Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals.},
	language = {en},
	number = {5},
	urldate = {2024-05-15},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Tzanetakis, G. and Cook, P.},
	month = jul,
	year = {2002},
	pages = {293--302},
	file = {Tzanetakis and Cook - 2002 - Musical genre classification of audio signals.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\GXUGY68K\\Tzanetakis and Cook - 2002 - Musical genre classification of audio signals.pdf:application/pdf},
}

@article{downie_music_2008,
	title = {The music information retrieval evaluation exchange (2005-2007): {A} window into music information retrieval research},
	volume = {29},
	issn = {1346-3969, 1347-5177},
	shorttitle = {The music information retrieval evaluation exchange (2005-2007)},
	url = {http://www.jstage.jst.go.jp/article/ast/29/4/29_4_247/_article},
	doi = {10.1250/ast.29.247},
	abstract = {The Music Information Retrieval Evaluation eXchange (MIREX) is the community-based framework for the formal evaluation of Music Information Retrieval (MIR) systems and algorithms. By looking at the background, structure, challenges, and contributions of MIREX this paper provides some insights into the world of MIR research. Because MIREX tasks are deﬁned by the community they reﬂect the interests, techniques, and research paradigms of the community as a whole. Both MIREX and MIR have a strong bias toward audio-based approaches as most MIR researchers have strengths in signal processing. Spectral-based approaches to MIR tasks have led to advancements in the MIR ﬁeld but they now appear to be reaching their limits of eﬀectiveness. This limitation is called the ‘‘glass ceiling’’ problem and the MIREX results data support its existence. The post-hoc analyses of MIREX results data indicate that there are groups of systems that perform equally well within various MIR tasks. There are many challenges facing MIREX and MIR research most of which have their root causes in the intellectual property issues surrounding music. The current inability of researchers to test their approaches against the MIREX test collections outside the annual MIREX cycle is hindering the rapid development of improved MIR systems.},
	language = {en},
	number = {4},
	urldate = {2024-05-15},
	journal = {Acoustical Science and Technology},
	author = {Downie, J. Stephen},
	year = {2008},
	pages = {247--255},
	file = {Downie - 2008 - The music information retrieval evaluation exchang.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\YU7J7H4J\\Downie - 2008 - The music information retrieval evaluation exchang.pdf:application/pdf},
}

@book{ras_advances_2010-1,
	address = {Berlin, Heidelberg},
	series = {Studies in {Computational} {Intelligence}},
	title = {Advances in {Music} {Information} {Retrieval}},
	volume = {274},
	copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
	isbn = {978-3-642-11673-5 978-3-642-11674-2},
	url = {https://link.springer.com/10.1007/978-3-642-11674-2},
	language = {en},
	urldate = {2024-05-15},
	publisher = {Springer Berlin Heidelberg},
	editor = {Raś, Zbigniew W. and Wieczorkowska, Alicja A. and Kacprzyk, Janusz},
	year = {2010},
	doi = {10.1007/978-3-642-11674-2},
	file = {Raś and Wieczorkowska - 2010 - Advances in Music Information Retrieval.pdf:C\:\\Users\\Konrad\\Zotero\\storage\\SBEVAAVG\\Raś and Wieczorkowska - 2010 - Advances in Music Information Retrieval.pdf:application/pdf},
}
